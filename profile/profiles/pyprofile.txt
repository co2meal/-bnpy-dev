Timer unit: 1 s

File: bnpy/HModel.py
Function: calc_local_params at line 67
Total time: 105.299 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    67                                                    @profile
    68                                                    def calc_local_params( self, Data, LP=None, **kwargs):
    69                                                      ''' Calculate the local parameters for each data item given global parameters.
    70                                                          This is the E-step of the EM/VB algorithm.        
    71                                                      '''
    72         3           0.000        0.000      0.0      if LP is None:
    73         1           0.000        0.000      0.0        LP = dict()
    74                                                      # Calculate the "soft evidence" each obsModel component has on each item
    75                                                      # Fills in LP['E_log_soft_ev']
    76         3           3.021        1.007      2.9      LP = self.obsModel.calc_local_params(Data, LP, **kwargs)
    77                                                      # Combine with allocModel probs of each cluster
    78                                                      # Fills in LP['resp'], a Data.nObs x K matrix whose rows sum to one
    79         3         102.278       34.093     97.1      LP = self.allocModel.calc_local_params(Data, LP, **kwargs)
    80         3           0.000        0.000      0.0      return LP


File: bnpy/HModel.py
Function: get_global_suff_stats at line 84
Total time: 0.575096 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    84                                                    @profile
    85                                                    def get_global_suff_stats( self, Data, LP, doAmplify=False, **kwargs):
    86                                                      ''' Calculate sufficient statistics for global parameters, given data and local responsibilities
    87                                                          This is necessary prep for the M-step of EM/VB.
    88                                                      '''
    89         4           0.017        0.004      2.9      SS = self.allocModel.get_global_suff_stats( Data, LP, **kwargs )
    90         4           0.558        0.140     97.1      SS = self.obsModel.get_global_suff_stats( Data, SS, LP, **kwargs )
    91         4           0.000        0.000      0.0      if doAmplify:
    92                                                        # Change effective scale (nObs) of the suff stats, for soVB learning
    93                                                        if hasattr(Data,"nDoc"):
    94                                                          ampF = Data.nDocTotal / Data.nDoc
    95                                                          SS.applyAmpFactor(ampF)
    96                                                        else:
    97                                                          ampF = Data.nObsTotal / Data.nObs
    98                                                          SS.applyAmpFactor(ampF)
    99         4           0.000        0.000      0.0      return SS


File: bnpy/HModel.py
Function: update_global_params at line 104
Total time: 3.51757 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   104                                                    @profile
   105                                                    def update_global_params( self, SS, rho=None, **kwargs):
   106                                                      ''' Update (in-place) global parameters given provided sufficient statistics.
   107                                                          This is the M-step of EM/VB.
   108                                                      '''
   109         3           3.242        1.081     92.2      self.allocModel.update_global_params(SS, rho, **kwargs)
   110         3           0.275        0.092      7.8      self.obsModel.update_global_params( SS, rho, **kwargs)


File: bnpy/HModel.py
Function: calc_evidence at line 115
Total time: 5.7245 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   115                                                    @profile
   116                                                    def calc_evidence( self, Data=None, SS=None, LP=None):
   117                                                      ''' Compute the evidence lower bound (ELBO) of the objective function.
   118                                                      '''
   119         3           0.000        0.000      0.0      if Data is not None and LP is None and SS is None:
   120                                                        LP = self.calc_local_params( Data )
   121                                                        SS = self.get_global_suff_stats( Data, LP)
   122         3           5.460        1.820     95.4      evA = self.allocModel.calc_evidence( Data, SS, LP)
   123         3           0.265        0.088      4.6      evObs = self.obsModel.calc_evidence( Data, SS, LP)
   124         3           0.000        0.000      0.0      return evA + evObs


File: bnpy/allocmodel/AllocModel.py
Function: update_global_params at line 62
Total time: 3.24149 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    62                                                    @profile
    63                                                    def update_global_params( self, SS, rho=None, **kwargs ):
    64                                                      ''' Update (in-place) global parameters for this allocation model object,
    65                                                          given the provided suff stats object SS
    66                                                          This is the M-step of EM/VB algorithm
    67                                                      '''
    68         3           0.000        0.000      0.0      self.K = SS.K
    69         3           0.000        0.000      0.0      if self.inferType == 'EM':
    70                                                        self.update_global_params_EM(SS)
    71         3           0.000        0.000      0.0      elif self.inferType == 'VB' or self.inferType == "moVB":
    72         3           3.241        1.080    100.0        self.update_global_params_VB(SS)
    73                                                      elif self.inferType == 'soVB':
    74                                                        if rho is None or rho==1:
    75                                                          self.update_global_params_VB(SS)
    76                                                        else: 
    77                                                          self.update_global_params_soVB(SS, rho)
    78                                                      else:
    79                                                        raise ValueError( 'Unrecognized Inference Type! %s' % (self.inferType) )
    80         3           0.000        0.000      0.0      self.isReady = True


File: bnpy/allocmodel/admix/HDPModel.py
Function: get_global_suff_stats at line 57
Total time: 0.01404 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    57                                                      @profile
    58                                                      def get_global_suff_stats(self, Data, LP, doPrecompEntropy=False, 
    59                                                                                                doPrecompMergeEntropy=False):
    60                                                          ''' Count expected number of times each topic is used across all docs    
    61                                                          '''
    62         4           0.000        0.000      0.1          wv = LP['word_variational']
    63         4           0.000        0.000      0.1          _, K = wv.shape
    64                                                          # Turn dim checking off, since some stats have dim K+1 instead of K
    65         4           0.008        0.002     54.2          SS = SuffStatDict(K=K, doCheck=False)
    66         4           0.001        0.000      4.7          SS.addScalar('nDoc', Data.nDoc)
    67         4           0.006        0.001     40.7          SS.sumLogPi = np.sum(LP['E_logPi'], axis=0)
    68         4           0.000        0.000      0.1          if doPrecompEntropy:
    69                                                              # Z terms
    70                                                              SS.addPrecompELBOTerm('ElogpZ', self.E_logpZ(Data, LP))
    71                                                              SS.addPrecompELBOTerm('ElogqZ', self.E_logqZ(Data, LP))
    72                                                              # Pi terms
    73                                                              # Note: no terms needed for ElogpPI
    74                                                              # SS already has field sumLogPi, which is sufficient for this term
    75                                                              ElogqPiConst, ElogqPiVec = self.E_logqPi_Memoized_from_LP(LP)
    76                                                              SS.addPrecompELBOTerm('ElogqPiConst', ElogqPiConst)
    77                                                              SS.addPrecompELBOTerm('ElogqPiVec', ElogqPiVec)
    78                                                  
    79         4           0.000        0.000      0.0          if doPrecompMergeEntropy:
    80                                                              ElogpZMat, sLgPiMat, ElogqPiMat = self.memo_elbo_terms_for_merge(LP)
    81                                                              ElogqZMat = self.E_logqZ_memo_terms_for_merge(Data, LP)
    82                                                              SS.addPrecompMergeTerm('ElogpZ', ElogpZMat)
    83                                                              SS.addPrecompMergeTerm('ElogqZ', ElogqZMat)
    84                                                              SS.addPrecompMergeTerm('ElogqPiVec', ElogqPiMat)
    85                                                              SS.addPrecompMergeTerm('sumLogPi', sLgPiMat)
    86         4           0.000        0.000      0.0          return SS


File: bnpy/allocmodel/admix/HDPModel.py
Function: calc_local_params at line 91
Total time: 101.257 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    91                                                      @profile
    92                                                      def calc_local_params( self, Data, LP, nCoordAscentIters=10):
    93                                                          ''' Calculate document-specific quantities (E-step)
    94                                                            Alternate updates to two terms until convergence
    95                                                              (1) Approx posterior on topic-token assignment
    96                                                                   q(word_variational | word_token_variables)
    97                                                              (2) Approx posterior on doc-topic probabilities
    98                                                                   q(doc_variational | document_topic_variables)
    99                                                  
   100                                                            Returns
   101                                                            -------
   102                                                            LP : local params dict, with fields
   103                                                                Pi : nDoc x K+1 matrix, 
   104                                                                   row d has params for doc d's Dirichlet over K+1 topics
   105                                                                word_variational : nDistinctWords x K matrix
   106                                                                   row i has params for word i's Discrete distr over K topics
   107                                                                DocTopicCount : nDoc x K matrix
   108                                                          '''
   109                                                          # When given no prev. local params LP, need to initialize from scratch
   110                                                          # this forces likelihood to drive the first round of local assignments
   111         3           0.000        0.000      0.0          if 'alphaPi' not in LP:
   112         1           0.001        0.001      0.0              LP['alphaPi'] = np.ones((Data.nDoc,self.K+1))
   113                                                          else:
   114         2           0.000        0.000      0.0              assert LP['alphaPi'].shape[1] == self.K + 1
   115                                                  
   116         3           0.085        0.028      0.1          LP = self.calc_ElogPi(LP)
   117         3           0.002        0.001      0.0          prevVec = LP['alphaPi'].flatten()
   118                                                          # Repeat until converged...
   119        33           0.000        0.000      0.0          for ii in xrange(nCoordAscentIters):
   120                                                              # Update word_variational field of LP
   121        30          94.780        3.159     93.6              LP = self.get_word_variational(Data, LP)
   122                                                          
   123                                                              # Update DocTopicCount field of LP
   124        30           0.035        0.001      0.0              LP['DocTopicCount'] = np.zeros((Data.nDoc,self.K))
   125    122700           0.229        0.000      0.2              for d in xrange(Data.nDoc):
   126    122670           0.781        0.000      0.8                  start,stop = Data.doc_range[d,:]
   127    122670           0.214        0.000      0.2                  LP['DocTopicCount'][d,:] = np.dot(
   128    122670           0.272        0.000      0.3                                             Data.word_count[start:stop],        
   129    122670           3.042        0.000      3.0                                             LP['word_variational'][start:stop,:]
   130                                                                                             )
   131                                                              # Update doc_variational field of LP
   132        30           0.111        0.004      0.1              LP = self.get_doc_variational(Data, LP)
   133        30           1.329        0.044      1.3              LP = self.calc_ElogPi(LP)
   134                                                  
   135                                                              # Assess convergence 
   136        30           0.022        0.001      0.0              curVec = LP['alphaPi'].flatten()
   137        30           0.332        0.011      0.3              if prevVec is not None and np.allclose(prevVec, curVec):
   138                                                                  print "converged after %d" % (ii)
   139                                                                  break
   140        30           0.022        0.001      0.0              prevVec = curVec
   141         3           0.000        0.000      0.0          return LP


File: bnpy/allocmodel/admix/HDPModel.py
Function: calc_ElogPi at line 143
Total time: 1.41295 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   143                                                      @profile
   144                                                      def calc_ElogPi(self, LP):
   145        33           0.000        0.000      0.0          alph = LP['alphaPi']
   146        33           1.413        0.043    100.0          LP['E_logPi'] = digamma(alph) - digamma(alph.sum(axis=1))[:,np.newaxis]
   147        33           0.000        0.000      0.0          return LP


File: bnpy/allocmodel/admix/HDPModel.py
Function: get_doc_variational at line 149
Total time: 0.085517 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   149                                                      @profile
   150                                                      def get_doc_variational( self, Data, LP):
   151                                                          ''' Update and return document-topic variational parameters
   152                                                          '''
   153        30           0.000        0.000      0.3          zeroPad = np.zeros((Data.nDoc,1))
   154        30           0.028        0.001     32.9          DTCountMatZeroPad = np.hstack([LP['DocTopicCount'], zeroPad])
   155        30           0.057        0.002     66.7          LP['alphaPi'] = DTCountMatZeroPad + self.gamma*self.Ebeta
   156        30           0.000        0.000      0.2          return LP


File: bnpy/allocmodel/admix/HDPModel.py
Function: get_word_variational at line 158
Total time: 94.4312 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   158                                                      @profile
   159                                                      def get_word_variational( self, Data, LP):
   160                                                          ''' Update and return word-topic assignment variational parameters
   161                                                          '''
   162                                                          # We call this wv_temp, since this will become the unnormalized
   163                                                          # variational parameter at the word level
   164        30          14.134        0.471     15.0          wv_temp = LP['E_logsoftev_WordsData'].copy() # so we can do += later
   165        30           0.000        0.000      0.0          K = wv_temp.shape[1]
   166        30           0.000        0.000      0.0          ElogPi = LP['E_logPi'][:,:K]
   167    122700           0.192        0.000      0.2          for d in xrange(Data.nDoc):
   168    122670           6.223        0.000      6.6              wv_temp[Data.doc_range[d,0]:Data.doc_range[d,1], :] += ElogPi[d,:]
   169        30           7.584        0.253      8.0          wv_temp -= np.max(wv_temp, axis=1)[:,np.newaxis]
   170        30          44.322        1.477     46.9          wv_temp = np.exp(wv_temp)
   171        30          18.474        0.616     19.6          wv_temp /= wv_temp.sum(axis=1)[:,np.newaxis]
   172        30           2.585        0.086      2.7          assert np.allclose(wv_temp.sum(axis=1), 1)
   173        30           0.916        0.031      1.0          LP['word_variational'] = wv_temp
   174        30           0.000        0.000      0.0          return LP


File: bnpy/allocmodel/admix/HDPModel.py
Function: calc_evidence at line 178
Total time: 5.45975 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   178                                                      @profile
   179                                                      def calc_evidence( self, Data, SS, LP ):
   180                                                          ''' Calculate ELBO terms related to allocation model
   181                                                          '''   
   182         3           0.007        0.002      0.1          E_logpV = self.E_logpV()
   183         3           0.001        0.000      0.0          E_logqV = self.E_logqV()
   184                                                       
   185         3           0.001        0.000      0.0          E_logpPi = self.E_logpPi(SS)
   186         3           0.000        0.000      0.0          if SS.hasPrecompELBO():
   187                                                            E_logqPi = SS.getPrecompELBOTerm('ElogqPiConst') \
   188                                                                        + np.sum(SS.getPrecompELBOTerm('ElogqPiVec'))
   189                                                            E_logpZ = np.sum(SS.getPrecompELBOTerm('ElogpZ'))
   190                                                            E_logqZ = np.sum(SS.getPrecompELBOTerm('ElogqZ'))
   191                                                          else:
   192         3           0.071        0.024      1.3            E_logqPi = self.E_logqPi(LP)
   193         3           0.008        0.003      0.1            E_logpZ = np.sum(self.E_logpZ(Data, LP))
   194         3           5.372        1.791     98.4            E_logqZ = np.sum(self.E_logqZ(Data, LP))
   195                                                  
   196         3           0.000        0.000      0.0          if SS.hasAmpFactor():
   197                                                              E_logqPi *= SS.ampF
   198                                                              E_logpZ *= SS.ampF
   199                                                              E_logqZ *= SS.ampF
   200                                                  
   201                                                          elbo = E_logpPi - E_logqPi \
   202                                                                 + E_logpZ - E_logqZ \
   203         3           0.000        0.000      0.0                 + E_logpV - E_logqV
   204         3           0.000        0.000      0.0          return elbo


File: bnpy/allocmodel/admix/HDPModel.py
Function: update_global_params_VB at line 333
Total time: 3.2407 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   333                                                      @profile
   334                                                      def update_global_params_VB(self, SS, **kwargs):
   335                                                          ''' Admixtures have no global allocation parameters! 
   336                                                              The mixture weights are document specific.
   337                                                          '''
   338         3           0.000        0.000      0.0          self.K = SS.K
   339         3           0.001        0.000      0.0          U1, U0 = HVO.estimate_u(K=self.K, alpha0=self.alpha0, gamma=self.gamma,
   340         3           3.239        1.080    100.0                       sumLogPi=SS.sumLogPi, nDoc=SS.nDoc)
   341         3           0.000        0.000      0.0          self.U1 = U1
   342         3           0.000        0.000      0.0          self.U0 = U0
   343         3           0.001        0.000      0.0          self.set_helper_params()


File: bnpy/allocmodel/admix/HDPVariationalOptimizer.py
Function: estimate_u at line 54
Total time: 3.23791 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    54                                                  @profile
    55                                                  def estimate_u(alpha0=1.0, gamma=0.5, nDoc=0, K=2, sumLogPi=None, Pi=None, doVerbose=False, **kwargs):
    56                                                    ''' Solve optimization problem to estimate parameters u
    57                                                        for the approximate posterior on stick-breaking fractions v
    58                                                        q(v | u) = Beta( v_k | u_k1, u_k0)
    59                                                  
    60                                                        Returns
    61                                                        -------
    62                                                        u1 : 
    63                                                        u0 : 
    64                                                    '''
    65         3           0.000        0.000      0.0    assert K + 1 == sumLogPi.size
    66         3           0.000        0.000      0.0    assert sumLogPi.ndim == 1
    67         3           0.000        0.000      0.0    if nDoc == 0:
    68                                                      initU = np.hstack( [0.1*np.ones(K), 0.1 * alpha0*np.ones(K)])
    69                                                      sumLogPi = np.zeros(K+1)
    70         3           0.000        0.000      0.0    elif Pi is not None:
    71                                                      logPi = np.maximum(np.log(Pi), -100)
    72                                                      sumLogPi = np.sum(logPi, axis=0)
    73                                                      initMeanBeta = np.mean(Pi, axis=0)
    74                                                      initMeanV = beta2v(initMeanBeta)
    75                                                      initSum = nDoc
    76                                                      initU = np.hstack( [initSum*initMeanV, initSum*(1-initMeanV)])
    77                                                      initU += 1 # so that it has a mode
    78                                                    else:
    79                                                      '''
    80                                                      initMeanBeta = np.exp(sumLogPi/nDoc)
    81                                                      initMeanBeta /= initMeanBeta.sum()
    82                                                      initMeanV = beta2v(initMeanBeta)
    83                                                      initSum = nDoc
    84                                                      initU = np.hstack( [initSum*initMeanV, initSum*(1-initMeanV)])
    85                                                      initU += 1 # so that it has a mode
    86                                                      '''
    87         3           0.001        0.000      0.0      initU = np.hstack( [np.ones(K), alpha0*np.ones(K)])      
    88                                                    
    89         3           0.000        0.000      0.0    if doVerbose:
    90                                                      print "INITIAL GUESS:"
    91                                                      print "   U1   : ", initU[:K]
    92                                                      print "   U0   : ", initU[K:]
    93                                                      initBeta = v2beta(initU[:K]/(initU[:K]+initU[K:]))
    94                                                      print "   E[beta] : ", initBeta
    95                                                    
    96         3           0.000        0.000      0.0    myFunc = lambda Cvec: objectiveFunc(Cvec, alpha0, gamma, nDoc, sumLogPi)
    97         3           0.000        0.000      0.0    myGrad = lambda Cvec: objectiveGradient(Cvec, alpha0, gamma, nDoc, sumLogPi)
    98                                                  
    99         3           3.237        1.079    100.0    bestCvec, bestf, Info = scipy.optimize.fmin_l_bfgs_b(myFunc, np.log(initU), fprime=myGrad, disp=None)
   100                                                    #bestCvec = scipy.optimize.fmin_bfgs(myFunc, np.log(initU), fprime=myGrad, disp=None)
   101         3           0.000        0.000      0.0    bestUvec = np.exp(bestCvec)
   102                                                  
   103         3           0.000        0.000      0.0    if np.allclose(bestUvec, initU):
   104                                                      print "WARNING: U estimation failed. Did not move from initial guess."
   105                                                  
   106         3           0.000        0.000      0.0    bestU1 = bestUvec[:K]
   107         3           0.000        0.000      0.0    bestU0 = bestUvec[K:]
   108         3           0.000        0.000      0.0    return bestU1, bestU0


File: bnpy/learn/VBLearnAlg.py
Function: fit at line 26
Total time: 114.013 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    26                                                    @profile
    27                                                    def fit(self, hmodel, Data):
    28                                                      # memoLPkeys : list of keys for LP that should be retained across laps
    29         1           0.000        0.000      0.0      self.memoLPkeys = hmodel.allocModel.get_keys_for_memoized_local_params()
    30         1           0.000        0.000      0.0      self.set_start_time_now()
    31         1           0.000        0.000      0.0      prevBound = -np.inf
    32         1           0.000        0.000      0.0      LP = None
    33         1           0.000        0.000      0.0      mergeFlags = dict(doPrecompEntropy=True, doPrecompMergeEntropy=True)
    34         4           0.000        0.000      0.0      for iterid in xrange(self.algParams['nLap'] + 1):
    35                                                        # M step
    36         3           0.000        0.000      0.0        if iterid > 0:
    37         2           2.256        1.128      2.0          hmodel.update_global_params(SS) 
    38         2           0.000        0.000      0.0          if self.hasMove('birth'):
    39                                                            hmodel, LP = self.run_birth_move(hmodel, Data, SS, LP, iterid)
    40                                                          
    41                                                        # E step 
    42         3         105.304       35.101     92.4        LP = hmodel.calc_local_params(Data, LP)
    43                                                  
    44         3           0.003        0.001      0.0        if self.hasMove('merge'):
    45                                                          SS = hmodel.get_global_suff_stats(Data, LP, **mergeFlags)
    46                                                        else:
    47         3           0.455        0.152      0.4          SS = hmodel.get_global_suff_stats(Data, LP)
    48                                                  
    49                                                        # ELBO calculation
    50         3           5.726        1.909      5.0        evBound = hmodel.calc_evidence(Data, SS, LP)
    51         3           0.000        0.000      0.0        if self.hasMove('merge'):
    52                                                          evBound2 = hmodel.calc_evidence(SS=SS)
    53                                                          assert np.allclose(evBound,evBound2)
    54                                                  
    55                                                        # Attempt merge move      
    56         3           0.000        0.000      0.0        if self.hasMove('merge'):
    57                                                          hmodel, SS, LP, evBound = self.run_merge_move(
    58                                                                                            hmodel, Data, SS, LP, evBound)
    59                                                  
    60                                                        # Save and display progress
    61         3           0.000        0.000      0.0        self.add_nObs(Data.nObsTotal)
    62         3           0.000        0.000      0.0        lap = iterid
    63         3           0.237        0.079      0.2        self.save_state(hmodel, iterid, lap, evBound)
    64         3           0.030        0.010      0.0        self.print_state(hmodel, iterid, lap, evBound)
    65                                                  
    66                                                        # Check for Convergence!
    67                                                        #  report warning if bound isn't increasing monotonically
    68         3           0.001        0.000      0.0        isConverged = self.verify_evidence( evBound, prevBound )
    69                                                  
    70         3           0.000        0.000      0.0        if isConverged:
    71                                                          break
    72         3           0.000        0.000      0.0        prevBound = evBound
    73                                                  
    74                                                      #Finally, save, print and exit
    75         1           0.000        0.000      0.0      if isConverged:
    76                                                        status = "converged."
    77                                                      else:
    78         1           0.000        0.000      0.0        status = "max passes thru data exceeded."
    79         1           0.000        0.000      0.0      self.save_state(hmodel,iterid, lap, evBound, doFinal=True)    
    80         1           0.000        0.000      0.0      self.print_state(hmodel,iterid, lap, evBound, doFinal=True, status=status)
    81         1           0.000        0.000      0.0      return LP, evBound


File: bnpy/obsmodel/MultObsModel.py
Function: get_global_suff_stats at line 57
Total time: 0.556051 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    57                                                      @profile
    58                                                      def get_global_suff_stats(self, Data, SS, LP, **kwargs):
    59                                                          ''' Calculate and return sufficient statistics.
    60                                                  
    61                                                              Returns
    62                                                              -------
    63                                                              SS : bnpy SuffStatDict object, with updated fields
    64                                                                  WordCounts : K x VocabSize matrix
    65                                                                    WordCounts[k,v] = # times vocab word v seen with topic k
    66                                                          '''
    67                                                          # Grab topic x word sufficient statistics
    68         4           0.000        0.000      0.0          wv = LP['word_variational']
    69         4           0.000        0.000      0.0          nDistinctWords, K = wv.shape
    70                                                  
    71                                                          # IMPROVED: sparse matrix product!
    72         4           0.012        0.003      2.1          WMat = Data.to_sparse_matrix()
    73         4           0.541        0.135     97.3          TopicWordCounts = (WMat * wv).T
    74                                                  
    75                                                          # OLD WAY: for loop
    76                                                          #TopicWordCounts = np.zeros((K, Data.vocab_size)) 
    77                                                          #effCount = wv * Data.word_count[:, np.newaxis]
    78                                                          #for ii in xrange(nDistinctWords):
    79                                                          #    TopicWordCounts[:, Data.word_id[ii]] += effCount[ii]
    80         4           0.000        0.000      0.0          SS.WordCounts = TopicWordCounts
    81         4           0.003        0.001      0.6          SS.N = np.sum(SS.WordCounts, axis=1)
    82         4           0.000        0.000      0.0          return SS


File: bnpy/obsmodel/MultObsModel.py
Function: calc_local_params at line 97
Total time: 3.02088 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    97                                                      @profile
    98                                                      def calc_local_params( self, Data, LP):
    99                                                          ''' Calculate local parameters (E-step)
   100                                                              For LDA, these are expectations for assigning each observed word
   101                                                              to all K possible topics.
   102                                                  
   103                                                              Returns
   104                                                              -------
   105                                                              LP : bnpy local parameter dict, with updated fields
   106                                                                  E_logsoftev_WordsData : nDistinctWords x K matrix, where
   107                                                                                  entry n,k = log p(word n | topic k)
   108                                                          '''
   109         3           0.000        0.000      0.0          if self.inferType == 'EM':
   110                                                              raise NotImplementedError('TODO')
   111                                                          else:
   112         3           3.021        1.007    100.0              LP['E_logsoftev_WordsData'] = self.E_logsoftev_WordsData(Data)
   113         3           0.000        0.000      0.0          return LP


File: bnpy/obsmodel/MultObsModel.py
Function: calc_evidence at line 137
Total time: 0.264477 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
   137                                                      @profile
   138                                                      def calc_evidence(self, Data, SS, LP):
   139         3           0.000        0.000      0.0          if self.inferType == 'EM':
   140                                                              return 0 # handled by alloc model
   141                                                          # Calculate p(w | z, lambda) + p(lambda) - q(lambda)
   142         3           0.096        0.032     36.5          elbo_pWords = self.E_log_pW(SS)
   143         3           0.016        0.005      6.1          elbo_pLambda = self.E_log_pLambda()
   144         3           0.152        0.051     57.4          elbo_qLambda = self.E_log_qLambda()
   145         3           0.000        0.000      0.0          lb_obs = elbo_pWords + elbo_pLambda - elbo_qLambda
   146                                                          
   147         3           0.000        0.000      0.0          return lb_obs


File: bnpy/obsmodel/ObsCompSet.py
Function: update_global_params at line 68
Total time: 0.274838 s

Line #      Hits            Time      Per Hit   % Time  Line Contents
=====================================================================
    68                                                    @profile
    69                                                    def update_global_params(self, SS, rho=None, Krange=None):
    70                                                      ''' M-step update of global parameters for each component of this obs model.
    71                                                          After this update, self will have exactly the number of 
    72                                                            components specified by SS.K.
    73                                                          If this number is changed, all components are rewritten from scratch.
    74                                                          Args
    75                                                          -------
    76                                                          SS : sufficient statistics object (bnpy.suffstats.SuffStatDict)
    77                                                          rho : learning rate for current step of stochastic online VB (soVB)
    78                                                  
    79                                                          Returns
    80                                                          -------
    81                                                          None (update happens *in-place*).         
    82                                                      '''
    83                                                      # TODO: if Krange specified, can we smartly do a component-specific update?
    84                                                  
    85                                                      # Components of updated model exactly match those of suff stats
    86         3           0.000        0.000      0.0      self.K = SS.K
    87         3           0.000        0.000      0.0      if len(self.comp) != self.K:
    88       101           0.027        0.000      9.9        self.comp = [copy.deepcopy(self.obsPrior) for k in xrange(self.K)]
    89         3           0.000        0.000      0.0      if Krange is None:
    90         3           0.000        0.000      0.0        Krange = xrange(self.K)
    91                                                  
    92         3           0.000        0.000      0.0      if self.inferType == 'EM':
    93                                                        self.update_obs_params_EM(SS, Krange)
    94         3           0.000        0.000      0.0      elif self.inferType.count('VB') > 0:
    95         3           0.000        0.000      0.0        if rho is None or rho == 1.0:
    96         3           0.248        0.083     90.1          self.update_obs_params_VB(SS, Krange)
    97                                                        else:
    98                                                          self.update_obs_params_soVB(SS, rho, Krange)


