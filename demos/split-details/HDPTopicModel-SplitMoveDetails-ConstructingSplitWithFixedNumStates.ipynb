{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bnpy\n",
    "reload(bnpy)\n",
    "calcSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats\n",
    "calcXSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split of single state into fixed set of states. \n",
    "\n",
    "We illustrate a split in four parts.\n",
    "\n",
    "1. First, showing example local parameters for a sample current configuration.\n",
    "2. Second, we show the proposed local parameters. *The construction method is left for later work, we only discuss the constraints the proposed parameters must abide relative to the originals.*\n",
    "3. Third, we show how to (trivially) obtain the relevant sufficient statistics for the proposal, via direct calculation. This is easy and affordable for small datasets, but in batch-by-batch processing we cannot touch all documents at once.\n",
    "4. Finally, we show how collecting batch-specific statistics and aggregating across batches, we can manipulate sufficient statistics to create whole-dataset proposal statistics **identical** to the direct method in step #3. \n",
    "\n",
    "\n",
    "### STEP 1: Current local parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider 3 existing states and 3 total documents. We imagine each document lives in its own batch, with its own value of beta, to simulate the fact that the entire vector beta may change from batch-to-batch."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        Doc-topic counts (N_dk)    Topic probabilities (beta_k)\n",
    "        k=1  k=2  k=3              k=1  k=2  k=3  remainder\n",
    "doc 1:   10   20   10              0.2  0.2  0.2  0.4\n",
    "doc 2:   50   30    5              0.4  0.1  0.1  0.4\n",
    "doc 3:    5   10    0              0.1  0.1  0.1  0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DocTopicCount = np.asarray([\n",
    "    [10, 20, 10], \n",
    "    [50, 30, 5], \n",
    "    [5, 10, 0],\n",
    "    ])\n",
    "Doc_beta = np.asarray([\n",
    "    [0.2, 0.2, 0.2], \n",
    "    [0.4, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.1],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = bnpy.data.WordsData(vocab_size=1, word_id=np.zeros(10), word_count=np.ones(10), doc_range=np.asarray([0,1,2,3]))\n",
    "theta = DocTopicCount + alpha * Doc_beta\n",
    "resp = np.random.rand(10,theta.shape[1])\n",
    "curLP = dict(DocTopicCount=DocTopicCount, theta=theta, resp=resp, thetaRem=alpha * (1-Doc_beta.sum(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, compute suff stats for the current set of components (before any expansion moves attempted)\n",
    "curSS = calcSummaryStats(Data, curLP, doPrecompEntropy=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Proposed local parameters\n",
    "\n",
    "Now, consider a split that divides *topic 1* into 2 topics, denoted \"1a\" and \"1b\". This yields one possible proposal:\n",
    "\n",
    "*Again, the important thing is not the specific values here, but the fact that we can illustrate how to create valid parameters theta for all documents that exactly represent a specific beta vector (that sums to one) and a specific set of assignments (for every word in the doc).*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        Doc-topic counts (N_dk)    Topic probabilities (beta_k)\n",
    "        k=1a  k=1b  k=2  k=3       k=1a  k=1b  k=2  k=3  remainder\n",
    "doc 1:     5     5   20   10        0.1   0.1  0.2  0.2  0.4\n",
    "doc 2:    40    10   30    5        0.3   0.1  0.1  0.1  0.4\n",
    "doc 3:     0     5   10    0        .09   .01  0.1  0.1  0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "Doc_beta = np.asarray([\n",
    "    [0.1, 0.1, 0.2, 0.2], \n",
    "    [0.3, 0.1, 0.1, 0.1],\n",
    "    [0.09, .01, 0.1, 0.1],\n",
    "    ])\n",
    "DocTopicCount = np.asarray([\n",
    "    [5, 5, 20, 10],\n",
    "    [40, 10, 30, 5],\n",
    "    [0, 5, 10, 0],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = bnpy.data.WordsData(vocab_size=1, word_id=np.zeros(10), word_count=np.ones(10), doc_range=np.asarray([0,1,2,3]))\n",
    "theta = DocTopicCount + alpha * Doc_beta\n",
    "resp = np.random.rand(10,theta.shape[1])\n",
    "propLP = dict(\n",
    "    DocTopicCount=DocTopicCount,\n",
    "    theta=theta,\n",
    "    resp=resp, \n",
    "    thetaRem=alpha * (1-Doc_beta.sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Constructing proposal statistics directly from the local parameters\n",
    "\n",
    "We can compute all the relevant sufficient statistics about the current and proposed value of theta.\n",
    "\n",
    "This is done compactly by an HDPTopicModel-specific function in bnpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "propSS_directFromLP = calcSummaryStats(Data, propLP, doPrecompEntropy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-17.22825291  -5.59231043  -2.26010056 -17.51404028]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect a few fields, just to understand\n",
    "\n",
    "# sumLogPi : aggregate log probability of each topic\n",
    "print propSS_directFromLP.sumLogPi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Constructing proposal statistics via aggregating and manipulating batch-specific stats\n",
    "\n",
    "Now, instead, we'll imagine that we created the proposal one batch at a time. Instead of tracking all the proposed thetas, at each document we just track summary statistics for the *expanded/split* states, that is, topics $1a$ and $1b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over all docs\n",
    "for d in range(Data.nDoc):\n",
    "    # Grab the single document\n",
    "    Data_b = Data.select_subset_by_mask([d])\n",
    "    # Create the local params just for this single document\n",
    "    propLP_newonly_b = dict(\n",
    "        DocTopicCount=DocTopicCount[d, :2][np.newaxis,:],\n",
    "        theta=theta[d, :2][np.newaxis,:],\n",
    "        resp=resp[d, :2][np.newaxis,:],\n",
    "        thetaRem=0,\n",
    "        )\n",
    "    propLP_newonly_b['digammaSumTheta'] = np.asarray([propLP['digammaSumTheta'][d]])\n",
    "    \n",
    "    # Create EXPANSION-ONLY statistics, which are useful for describing only the newly proposed states\n",
    "    propSS_newonly_b = calcXSummaryStats(Data_b, propLP_newonly_b, doPrecompEntropy=1, uids=[101, 102])\n",
    "    \n",
    "    # Aggregate the stats across this loop, so in the end all documents are represented.\n",
    "    if d == 0:\n",
    "        propSS_newonly = propSS_newonly_b.copy()\n",
    "    else:\n",
    "        propSS_newonly += propSS_newonly_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, we use suff stat manipulation to transform \n",
    "# from the current stats (K=3)\n",
    "# into valid stats representing the proposal (K=4)\n",
    "propSS_fromXSS = curSS.copy()\n",
    "propSS_fromXSS.replaceCompWithExpansion(uid=0, xSS=propSS_newonly)\n",
    "# This expanded proposal always places the new comps last in order, so let's shuffle our original proposal that way too\n",
    "propSS_directFromLP.reorderComps([2,3,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumLogPi\n",
      "  [ -2.26010056 -17.51404028 -17.22825291  -5.59231043]  direct construction proposal\n",
      "  [ -2.26010056 -17.51404028 -17.22825291  -5.59231043]  from tracked expansion stats\n",
      "sumLogPiRem\n",
      "  -17.2336554208  direct construction proposal\n",
      "  -17.2336554208  from tracked expansion stats\n",
      "gammalnTheta\n",
      "  [ 124.55818972   18.83674356  113.42794164   19.55041697]  direct construction proposal\n",
      "  [ 124.55818972   18.83674356  113.42794164   19.55041697]  from tracked expansion stats\n",
      "gammalnSumTheta\n",
      "  433.986512449  direct construction proposal\n",
      "  433.986512449  from tracked expansion stats\n",
      "slackTheta\n",
      "  [ 0.2980702   1.89427998  1.732848    0.44828941]  direct construction proposal\n",
      "  [ 0.2980702   1.89427998  1.732848    0.44828941]  from tracked expansion stats\n",
      "slackThetaRem\n",
      "  8.08177323293  direct construction proposal\n",
      "  8.08177323293  from tracked expansion stats\n"
     ]
    }
   ],
   "source": [
    "for key in ['sumLogPi', 'sumLogPiRem', 'gammalnTheta', 'gammalnSumTheta', 'slackTheta', 'slackThetaRem']:\n",
    "    print key\n",
    "    if hasattr(propSS_fromXSS, key):\n",
    "        arr_directFromLP = getattr(propSS_directFromLP, key)\n",
    "        arr_fromXSS = getattr(propSS_fromXSS, key)\n",
    "        \n",
    "    elif propSS_fromXSS.hasELBOTerm(key):\n",
    "        arr_directFromLP = propSS_directFromLP.getELBOTerm(key)\n",
    "        arr_fromXSS = propSS_fromXSS.getELBOTerm(key)\n",
    "        \n",
    "    print '  %s  direct construction proposal' % (arr_directFromLP)\n",
    "    print '  %s  from tracked expansion stats' % (arr_fromXSS)\n",
    "    assert np.allclose(arr_fromXSS, arr_directFromLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
