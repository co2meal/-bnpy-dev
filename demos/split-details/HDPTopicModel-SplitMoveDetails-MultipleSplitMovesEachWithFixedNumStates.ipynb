{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bnpy\n",
    "reload(bnpy)\n",
    "calcSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats\n",
    "calcXSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying two split moves (one on topic 2, another on 3) into fixed set of states. \n",
    "\n",
    "We illustrate a split in four parts.\n",
    "\n",
    "1. First, showing example local parameters for a sample current configuration.\n",
    "2. Second, we show the proposed local parameters. *The construction method is left for later work, we only discuss the constraints the proposed parameters must abide relative to the originals.*\n",
    "3. Third, we show how to (trivially) obtain the relevant sufficient statistics for the proposal, via direct calculation. This is easy and affordable for small datasets, but in batch-by-batch processing we cannot touch all documents at once.\n",
    "4. Finally, we show how collecting batch-specific statistics and aggregating across batches, we can manipulate sufficient statistics to create whole-dataset proposal statistics **identical** to the direct method in step #3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Original parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataset with 3 docs. word content doesnt matter, since we just focus on upper-level inference of topic probs\n",
    "Data = bnpy.data.WordsData(vocab_size=1, word_id=np.zeros(10), word_count=np.ones(10), doc_range=np.asarray([0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curDoc_beta = np.asarray([\n",
    "    [0.2, 0.2, 0.2], \n",
    "    [0.4, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.1],\n",
    "    ])\n",
    "\n",
    "curDocTopicCount = np.asarray([\n",
    "    [10, 20, 10], \n",
    "    [50, 30, 5], \n",
    "    [5, 10, 0],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curDoc_betaRem = 1 - curDoc_beta.sum(axis=1)\n",
    "curTheta = curDocTopicCount + alpha * curDoc_beta\n",
    "curThetaRem = alpha * curDoc_betaRem\n",
    "curdigammaSumTheta = digamma(np.sum(curTheta + curThetaRem, axis=1))\n",
    "curLP = dict(\n",
    "    DocTopicCount=curDocTopicCount,\n",
    "    theta=curTheta,\n",
    "    thetaRem=curThetaRem,\n",
    "    resp=np.random.rand(Data.nUniqueToken, curTheta.shape[1]),\n",
    "    digammaSumTheta=curdigammaSumTheta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curSS = calcSummaryStats(Data, curLP, doPrecompEntropy=1, doTrackTruncationGrowth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Create proposal for expansion of topic 2 and topic 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split of topic 2 into three new topics\n",
    "xDoc_beta_2 = np.asarray([\n",
    "    [0.1, 0.05, .05],\n",
    "    [0.07, 0.01, .02],\n",
    "    [0.02, 0.02, 0.06],\n",
    "    ])\n",
    "xDocTopicCount_2 = np.asarray([\n",
    "    [10, 5, 5],\n",
    "    [5, 5, 20],\n",
    "    [3, 3, 4],\n",
    "    ])\n",
    "assert np.allclose(xDoc_beta_2.sum(axis=1), curDoc_beta[:, 1])\n",
    "assert np.allclose(xDocTopicCount_2.sum(axis=1), curDocTopicCount[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split of topic 3 into 3 new topics\n",
    "xDoc_beta_3 = np.asarray([\n",
    "    [0.1, 0.05, .05],     # sums to 0.2\n",
    "    [0.07, 0.01, .02],    # sums to 0.1\n",
    "    [0.02, 0.04, 0.04],   # sums to 0.1 = curBeta[doc=3, k=3]\n",
    "    ])\n",
    "xDocTopicCount_3 = np.asarray([\n",
    "    [10, 0, 0],   # sums to 10 = curDocTopicCount[doc=1, k=3]\n",
    "    [0, 3, 2],  # sums to 5\n",
    "    [0, 0, 0],    # sums to 0\n",
    "    ])\n",
    "assert np.allclose(xDoc_beta_3.sum(axis=1), curDoc_beta[:, 2])\n",
    "assert np.allclose(xDocTopicCount_3.sum(axis=1), curDocTopicCount[:, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create suff stats for for *combined* proposal, expanding topic 2 and topic 3 directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keepCompIDs = [0]\n",
    "\n",
    "propDocTopicCount = np.hstack([curDocTopicCount[:, keepCompIDs], xDocTopicCount_2, xDocTopicCount_3])\n",
    "propDoc_beta = np.hstack([curDoc_beta[:, keepCompIDs], xDoc_beta_2, xDoc_beta_3])\n",
    "\n",
    "assert np.allclose(propDocTopicCount.sum(axis=1),\n",
    "                   curDocTopicCount.sum(axis=1))\n",
    "assert np.allclose(propDoc_beta.sum(axis=1), \n",
    "                   curDoc_beta.sum(axis=1))\n",
    "\n",
    "propK = propDoc_beta.shape[1]\n",
    "propTheta = propDocTopicCount + alpha * propDoc_beta\n",
    "propResp = np.random.rand(10, propK)\n",
    "propLP = dict(\n",
    "    DocTopicCount=propDocTopicCount,\n",
    "    theta=propTheta,\n",
    "    resp=propResp, \n",
    "    thetaRem=curThetaRem,\n",
    "    digammaSumTheta=curLP['digammaSumTheta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "propSS_directFromLP = calcSummaryStats(Data, propLP, doPrecompEntropy=1, doTrackTruncationGrowth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create suff stats for *combined* proposal, by stitching together stats for each individual move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over all docs\n",
    "for d in range(Data.nDoc):\n",
    "    # Grab the single document\n",
    "    Data_b = Data.select_subset_by_mask([d])\n",
    "    # Do local and summary step using only expansion terms from state 2\n",
    "    propLP_newonly_2b = dict(\n",
    "        DocTopicCount=xDocTopicCount_2[d,:][np.newaxis,:],\n",
    "        theta=xDocTopicCount_2[d,:][np.newaxis,:] + alpha * xDoc_beta_2[d,:][np.newaxis,:],\n",
    "        resp=np.random.rand(1, xDocTopicCount_2.shape[1]), # doesnt matter\n",
    "        thetaRem=curThetaRem[d],\n",
    "        )\n",
    "    propLP_newonly_2b['digammaSumTheta'] = np.asarray([curLP['digammaSumTheta'][d]])\n",
    "    propSS_newonly_2b = calcXSummaryStats(Data_b, propLP_newonly_2b, uids=[200,201],\n",
    "                                          doPrecompEntropy=1, doTrackTruncationGrowth=1)\n",
    "    # Do local and summary step using only expansion terms from state 3\n",
    "    propLP_newonly_3b = dict(\n",
    "        DocTopicCount=xDocTopicCount_3[d,:][np.newaxis,:],\n",
    "        theta=xDocTopicCount_3[d,:][np.newaxis,:] + alpha * xDoc_beta_3[d,:][np.newaxis,:],\n",
    "        resp=np.random.rand(1, xDocTopicCount_3.shape[1]), # doesnt matter\n",
    "        thetaRem=curThetaRem[d],\n",
    "        )\n",
    "    propLP_newonly_3b['digammaSumTheta'] = np.asarray([curLP['digammaSumTheta'][d]])\n",
    "    propSS_newonly_3b = calcXSummaryStats(Data_b, propLP_newonly_3b, uids=[300,301],\n",
    "                                          doPrecompEntropy=1, doTrackTruncationGrowth=1)\n",
    "    # Aggregate the stats across this loop, so in the end all documents are represented.\n",
    "    if d == 0:\n",
    "        propSS_newonly_2 = propSS_newonly_2b.copy()\n",
    "        propSS_newonly_3 = propSS_newonly_3b.copy()\n",
    "    else:\n",
    "        propSS_newonly_2 += propSS_newonly_2b\n",
    "        propSS_newonly_3 += propSS_newonly_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we use suff stat manipulation to transform \n",
    "# from the current stats (K=3)\n",
    "# into valid stats representing the proposal (K=4)\n",
    "propSS_fromXSS = curSS.copy()\n",
    "propSS_fromXSS.replaceCompWithExpansion(uid=1, xSS=propSS_newonly_2)\n",
    "propSS_fromXSS.replaceCompWithExpansion(uid=2, xSS=propSS_newonly_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumLogPi\n",
      "  [ -3.18  -6.18  -6.93  -5.13 -73.93 -55.98 -56.47]  direct construction proposal\n",
      "  [ -3.18  -6.18  -6.93  -5.13 -73.93 -55.98 -56.47]  from tracked expansion stats\n",
      "sumLogPiRemVec\n",
      "  [  0.     0.     0.     0.     0.     0.   -17.23]  direct construction proposal\n",
      "  [  0.     0.     0.     0.     0.     0.   -17.23]  from tracked expansion stats\n",
      "gammalnTheta\n",
      "  [ 162.71   17.02    7.16   44.52   19.55    6.87    6.17]  direct construction proposal\n",
      "  [ 162.71   17.02    7.16   44.52   19.55    6.87    6.17]  from tracked expansion stats\n",
      "gammalnSumTheta\n",
      "  433.986512449  direct construction proposal\n",
      "  433.986512449  from tracked expansion stats\n",
      "slackTheta\n",
      "  [ 0.62  0.39  0.17  0.23  2.55  2.38  2.42]  direct construction proposal\n",
      "  [ 0.62  0.39  0.17  0.23  2.55  2.38  2.42]  from tracked expansion stats\n",
      "slackThetaRem\n",
      "  8.08177323293  direct construction proposal\n",
      "  8.08177323293  from tracked expansion stats\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=100, precision=2)\n",
    "for key in ['sumLogPi', 'sumLogPiRemVec', 'gammalnTheta', 'gammalnSumTheta', 'slackTheta', 'slackThetaRem']:\n",
    "    print key\n",
    "    if hasattr(propSS_fromXSS, key):\n",
    "        arr_directFromLP = getattr(propSS_directFromLP, key)\n",
    "        arr_fromXSS = getattr(propSS_fromXSS, key)\n",
    "        \n",
    "    elif propSS_fromXSS.hasELBOTerm(key):\n",
    "        arr_directFromLP = propSS_directFromLP.getELBOTerm(key)\n",
    "        arr_fromXSS = propSS_fromXSS.getELBOTerm(key)\n",
    "        \n",
    "    print '  %s  direct construction proposal' % (arr_directFromLP)\n",
    "    print '  %s  from tracked expansion stats' % (arr_fromXSS)\n",
    "    assert np.allclose(arr_fromXSS, arr_directFromLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
