{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma\n",
    "\n",
    "np.set_printoptions(formatter=dict(all=lambda s: '% 7.3f' % (s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bnpy\n",
    "reload(bnpy)\n",
    "calcSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats\n",
    "calcXSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats_expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split of single state into a growing set of states. \n",
    "\n",
    "We illustrate a growing-split in four parts.\n",
    "\n",
    "1. First, showing example local parameters for a hypothetical current configuration.\n",
    "2. Second, we show the proposed local parameters for several batches. *The construction method is left for later work, we only care that the proposed parameters obey special constraints relative to the originals.*\n",
    "3. Third, we show how to (trivially) obtain the relevant sufficient statistics for the proposal, via direct calculation. This is easy and affordable for small datasets, but in batch-by-batch processing we cannot touch all documents at once.\n",
    "4. Finally, we show how collecting batch-specific statistics and aggregating across batches, we can manipulate sufficient statistics to create whole-dataset proposal statistics **identical** to the direct method in step #3. \n",
    "\n",
    "\n",
    "### STEP 1: Current local parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider 3 existing states and 3 total documents. We imagine each document lives in its own batch, with its own value of beta, to simulate the fact that the entire vector beta may change from batch-to-batch."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        Doc-topic counts (N_dk)    Topic probabilities (beta_k)\n",
    "        k=1  k=2  k=3              k=1  k=2  k=3  remainder\n",
    "doc 1:   10   20   10              0.2  0.2  0.2  0.4\n",
    "doc 2:   50   30    5              0.4  0.1  0.1  0.4\n",
    "doc 3:    5   10    0              0.1  0.1  0.1  0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curDocTopicCount = np.asarray([\n",
    "    [10, 20, 10], \n",
    "    [50, 30, 5], \n",
    "    [5, 10, 0],\n",
    "    ])\n",
    "curDoc_beta = np.asarray([\n",
    "    [0.2, 0.2, 0.2], \n",
    "    [0.4, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.1],\n",
    "    ])\n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = bnpy.data.WordsData(vocab_size=1, word_id=np.zeros(10), word_count=np.ones(10), doc_range=np.asarray([0,1,2,3]))\n",
    "theta = curDocTopicCount + alpha * curDoc_beta\n",
    "resp = np.random.rand(10,theta.shape[1])\n",
    "curLP = dict(DocTopicCount=curDocTopicCount, theta=theta, resp=resp, thetaRem=alpha * (1-curDoc_beta.sum(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, compute suff stats for the current set of components (before any expansion moves attempted)\n",
    "curSS = calcSummaryStats(Data, curLP, doPrecompEntropy=1, doTrackTruncationGrowth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Proposed local parameters\n",
    "\n",
    "Now, consider a split that acts on topic 1. We may assign its mass to two topics, \"1a\" and \"1b\", in the first batch, then add an additional topic \"1c\" in the 2nd batch, and a final topic in the third batch.\n",
    "\n",
    "*Again, the important thing is not the specific values here, but the fact that we can illustrate how to create valid parameters theta for all documents that exactly represent a specific beta vector (that sums to one) and a specific set of assignments (for every word in the doc).*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        Doc-topic counts (N_dk)\n",
    "        k=2  k=3  k=1a  k=1b  k=1c k=1d\n",
    "doc 1:   20   10     5     5   N/A  N/A   1a-1d sum to 10 (the current value for state 1)\n",
    "doc 2:   30    5    25     5    20  N/A\n",
    "doc 3:   10    0     0     0     1    4\n",
    "\n",
    "        Topic probabilities (beta)\n",
    "        k=2  k=3  k=1a  k=1b  k=1c k=1d\n",
    "doc 1:  0.2  0.2  0.05  0.05  N/A  N/A     1a-1d need to sum to 0.2 (the current value for state 1)\n",
    "doc 2:  0.1  0.1  0.1   0.1   0.2  N/A     1a-1d need to sum to 0.4 (the current value for state 1)\n",
    "doc 3:  0.1  0.1  .01   .01   .03  .04     1a-1d need to sum to 0.1 (the current value for state 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "Doc_beta = np.asarray([\n",
    "    [0.2,  0.2,  0.15,  0.05,  0,    0],\n",
    "    [0.1,  0.1,  0.1,   0.1,   0.2,  0],\n",
    "    [0.1,  0.1,  .01,   .02,   .03,  .04]\n",
    "    ])\n",
    "DocTopicCount = np.asarray([\n",
    "    [20, 10, 5, 5,  0, 0],\n",
    "    [30, 5, 25, 5, 20, 0],\n",
    "    [10, 0, 0,  0, 1,  4],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = bnpy.data.WordsData(vocab_size=1, word_id=np.zeros(10), word_count=np.ones(10), doc_range=np.asarray([0,1,2,3]))\n",
    "theta = DocTopicCount + alpha * Doc_beta\n",
    "resp = np.random.rand(10,theta.shape[1])\n",
    "thetaRem=alpha * (1-Doc_beta.sum(axis=1))\n",
    "K_d = np.asarray([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.600   0.600   0.300]\n",
      "[  0.600   0.600   0.300]\n",
      "[ 40.000  85.000  15.000]\n",
      "[ 40.000  85.000  15.000]\n"
     ]
    }
   ],
   "source": [
    "# Verify that Doc_beta and curDoc_beta have same sums\n",
    "print Doc_beta.sum(axis=1)\n",
    "print curDoc_beta.sum(axis=1)\n",
    "\n",
    "print DocTopicCount.sum(axis=1)\n",
    "print curDocTopicCount.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Constructing proposal statistics directly from the local parameters\n",
    "\n",
    "We can compute all the relevant sufficient statistics about the current and proposed value of theta.\n",
    "\n",
    "This is done compactly by an HDPTopicModel-specific function in bnpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over all docs\n",
    "for d in range(Data.nDoc):\n",
    "    # Grab the single document\n",
    "    Data_b = Data.select_subset_by_mask([d])\n",
    "    \n",
    "    # Create the local params just for this single document\n",
    "    propLP_b = dict(\n",
    "        DocTopicCount=DocTopicCount[d, :K_d[d]][np.newaxis,:],\n",
    "        theta=theta[d, :K_d[d]][np.newaxis,:],\n",
    "        resp=resp[d, :K_d[d]][np.newaxis,:],\n",
    "        thetaRem=thetaRem[d],\n",
    "        )\n",
    "    propSS_b = calcSummaryStats(Data_b, propLP_b, doPrecompEntropy=1, doTrackTruncationGrowth=1)\n",
    "    \n",
    "    # Aggregate the stats across this loop, so in the end all documents are represented.\n",
    "    if d == 0:\n",
    "        propSS = propSS_b.copy()\n",
    "    else:\n",
    "        Kextra = propSS_b.K - propSS.K\n",
    "        if Kextra > 0:\n",
    "            propSS.insertEmptyComps(Kextra)\n",
    "        propSS += propSS_b\n",
    "propSS_directFromLP = propSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.260 -17.514 -106.710 -58.391  -4.738  -1.474]\n",
      "[  0.000   0.000   0.000  -6.263  -7.010  -3.961]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect a few fields, just to understand\n",
    "\n",
    "# sumLogPi : aggregate log probability of each topic\n",
    "print propSS_directFromLP.sumLogPi\n",
    "\n",
    "# sumLogPiRemVec : aggregate \"remainder topic\" probabilities\n",
    "print propSS_directFromLP.sumLogPiRemVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sumLogPiRemVec is the \"remainder\" mass for topics larger than the assigned truncation level.\n",
    "\n",
    "When we allow the truncation to grow at each batch, we need to track the fact that we had X leftover mass after K=4 at document 1, then Y leftover mass after K=5 at document 2, etc.\n",
    "\n",
    "So, we keep a vector of size K, where entry k is the aggregate total (across all documents) of the leftover mass beyond topic k for any document with truncation K=k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Constructing proposal statistics via aggregating and manipulating batch-specific stats\n",
    "\n",
    "Now, instead, we'll imagine that we created the proposal one batch at a time. Instead of tracking all the proposed thetas, at each document we just track summary statistics for the *expanded/split* states, that is, topics $1a$ and $1b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over all docs\n",
    "for d in range(Data.nDoc):\n",
    "    # Grab the single document\n",
    "    Data_b = Data.select_subset_by_mask([d])\n",
    "    \n",
    "    # Create the local params just for this single document\n",
    "    digammaSumTheta_b = np.asarray([digamma(theta[d, :K_d[d]].sum() + thetaRem[d])])\n",
    "    propLP_b_newonly = dict(\n",
    "        DocTopicCount=DocTopicCount[d, 2:K_d[d]][np.newaxis,:],\n",
    "        theta=theta[d, 2:K_d[d]][np.newaxis,:],\n",
    "        resp=resp[d, 2:K_d[d]][np.newaxis,:],\n",
    "        thetaRem=thetaRem[d],\n",
    "        digammaSumTheta=digammaSumTheta_b,        \n",
    "        )\n",
    "    propSS_b_newonly = calcXSummaryStats(\n",
    "        Data_b, propLP_b_newonly,\n",
    "        doPrecompEntropy=1, doTrackTruncationGrowth=1,\n",
    "        uids=np.arange(3, K_d[d]))\n",
    "    \n",
    "    # Aggregate the stats across this loop, so in the end all documents are represented.\n",
    "    if d == 0:\n",
    "        propSS_newonly = propSS_b_newonly.copy()\n",
    "    else:\n",
    "        Kextra = propSS_b_newonly.K - propSS_newonly.K\n",
    "        if Kextra > 0:\n",
    "            propSS_newonly.insertEmptyComps(Kextra)\n",
    "        propSS_newonly += propSS_b_newonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we use suff stat manipulation to transform \n",
    "# from the current stats (K=3)\n",
    "# into valid stats representing the proposal (K=4)\n",
    "propSS_fromXSS = curSS.copy()\n",
    "propSS_fromXSS.replaceCompWithExpansion(uid=0, xSS=propSS_newonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumLogPi\n",
      "  [ -2.260 -17.514 -106.710 -58.391  -4.738  -1.474]  direct construction proposal\n",
      "  [ -2.260 -17.514 -106.710 -58.391  -4.738  -1.474]  from tracked expansion stats\n",
      "sumLogPiRemVec\n",
      "  [  0.000   0.000   0.000  -6.263  -7.010  -3.961]  direct construction proposal\n",
      "  [  0.000   0.000   0.000  -6.263  -7.010  -3.961]  from tracked expansion stats\n",
      "gammalnTheta\n",
      "  [ 124.558  18.837  63.111  10.484  39.918   1.842]  direct construction proposal\n",
      "  [ 124.558  18.837  63.111  10.484  39.918   1.842]  from tracked expansion stats\n",
      "gammalnSumTheta\n",
      "  433.986512449  direct construction proposal\n",
      "  433.986512449  from tracked expansion stats\n",
      "slackTheta\n",
      "  [  0.298   1.894   1.482   1.467   0.392   0.059]  direct construction proposal\n",
      "  [  0.298   1.894   1.482   1.467   0.392   0.059]  from tracked expansion stats\n",
      "slackThetaRem\n",
      "  8.08177323293  direct construction proposal\n",
      "  8.08177323293  from tracked expansion stats\n"
     ]
    }
   ],
   "source": [
    "for key in ['sumLogPi', 'sumLogPiRemVec', 'gammalnTheta', 'gammalnSumTheta', 'slackTheta', 'slackThetaRem']:\n",
    "    print key\n",
    "    if hasattr(propSS_fromXSS, key):\n",
    "        arr_directFromLP = getattr(propSS_directFromLP, key)\n",
    "        arr_fromXSS = getattr(propSS_fromXSS, key)\n",
    "        \n",
    "    elif propSS_fromXSS.hasELBOTerm(key):\n",
    "        arr_directFromLP = propSS_directFromLP.getELBOTerm(key)\n",
    "        arr_fromXSS = propSS_fromXSS.getELBOTerm(key)\n",
    "        \n",
    "    print '  %s  direct construction proposal' % (arr_directFromLP)\n",
    "    print '  %s  from tracked expansion stats' % (arr_fromXSS)\n",
    "    assert np.allclose(arr_fromXSS, arr_directFromLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
