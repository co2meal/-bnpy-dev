{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bnpy\n",
    "reload(bnpy)\n",
    "calcSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats\n",
    "calcXSummaryStats = bnpy.allocmodel.topics.HDPTopicModel.calcSummaryStats_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying two splits (one on topic 2, another on 3), each growing in size.\n",
    "\n",
    "We illustrate a split in four parts.\n",
    "\n",
    "1. First, showing example local parameters for a sample current configuration.\n",
    "2. Second, we show the proposed local parameters. *The construction method is left for later work, we only discuss the constraints the proposed parameters must abide relative to the originals.*\n",
    "3. Third, we show how to (trivially) obtain the relevant sufficient statistics for the proposal, via direct calculation. This is easy and affordable for small datasets, but in batch-by-batch processing we cannot touch all documents at once.\n",
    "4. Finally, we show how collecting batch-specific statistics and aggregating across batches, we can manipulate sufficient statistics to create whole-dataset proposal statistics **identical** to the direct method in step #3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Original parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset with 3 docs. word content doesnt matter, since we just focus on upper-level inference of topic probs\n",
    "Data = bnpy.data.WordsData(vocab_size=1, word_id=np.zeros(3), word_count=np.ones(3), doc_range=np.asarray([0,1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current truncation: K=3 topics.  We show the beta vector (probabilities of active topics) and document-topic assignment counts, for each of the 3 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curDoc_beta = np.asarray([\n",
    "    [0.2, 0.2, 0.2], \n",
    "    [0.4, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.1],\n",
    "    ])\n",
    "\n",
    "curDocTopicCount = np.asarray([\n",
    "    [10, 20, 10], \n",
    "    [50, 30, 0], \n",
    "    [5, 10, 9],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curDoc_betaRem = 1 - curDoc_beta.sum(axis=1)\n",
    "curTheta = curDocTopicCount + alpha * curDoc_beta\n",
    "curThetaRem = alpha * curDoc_betaRem\n",
    "curdigammaSumTheta = digamma(np.sum(curTheta + curThetaRem, axis=1))\n",
    "curLP = dict(\n",
    "    DocTopicCount=curDocTopicCount,\n",
    "    theta=curTheta,\n",
    "    thetaRem=curThetaRem,\n",
    "    resp=np.random.rand(Data.nUniqueToken, curTheta.shape[1]),\n",
    "    digammaSumTheta=curdigammaSumTheta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curSS = calcSummaryStats(Data, curLP, doPrecompEntropy=1, doTrackTruncationGrowth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Create proposal for expansion of topic 2 and topic 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split of topic 2\n",
    "\n",
    "# Truncation grows from Korig topics,\n",
    "# to K=Korig+2 new topics at doc 1,\n",
    "# to K=Korig+3 at doc 2, \n",
    "# to K=Korig+4 at doc 3\n",
    "K_d_2 = np.asarray([2, 3, 4])\n",
    "\n",
    "xDoc_beta_2 = np.asarray([\n",
    "    [0.09,  0.11,  0,    0],    # sums to 0.2\n",
    "    [0.07, 0.01,  0.02, 0],    # sums to 0.1\n",
    "    [0.02, 0.02,  0.02,  0.04], # sums to 0.1\n",
    "    ])\n",
    "xDocTopicCount_2 = np.asarray([\n",
    "    [10, 10, 0,  0],   # sums to 20\n",
    "    [5,   0, 25, 0],   # sums to 30\n",
    "    [0,   3,  3, 4],    # sums to 10\n",
    "    ])\n",
    "assert np.allclose(xDoc_beta_2.sum(axis=1), curDoc_beta[:, 1])\n",
    "assert np.allclose(xDocTopicCount_2.sum(axis=1), curDocTopicCount[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split of topic 3\n",
    "\n",
    "# Truncation grows from Korig topics,\n",
    "# to K=Korig+2 new topics at doc 1,\n",
    "# to K=Korig+2 at doc 2, \n",
    "# to K=Korig+4 at doc 3\n",
    "K_d_3 = np.asarray([2, 2, 4])\n",
    "\n",
    "xDoc_beta_3 = np.asarray([\n",
    "    [0.1,  0.10,  0,    0],    # sums to 0.2\n",
    "    [0.07, 0.03,  0,    0],    # sums to 0.1\n",
    "    [0.02, 0.02,  0.03,  0.03], # sums to 0.1\n",
    "    ])\n",
    "\n",
    "xDocTopicCount_3 = np.asarray([\n",
    "    [3, 7, 0,  0],   # sums to 10\n",
    "    [0,   0, 0, 0],   # sums to 0\n",
    "    [0,   3,  3, 3],    # sums to 9\n",
    "    ])\n",
    "assert np.allclose(xDoc_beta_3.sum(axis=1), curDoc_beta[:, 2])\n",
    "assert np.allclose(xDocTopicCount_3.sum(axis=1), curDocTopicCount[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create suff stats for *combined* proposal, expanding topic 2 and topic 3 directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sumLogPiRem for doc 1, K[d]=5\n",
      "------\n",
      "2and3 :  [ 0.    0.    0.    0.   -6.26]\n",
      " only2:  [ 0.   -6.26]\n",
      " only3:  [ 0.   -6.26]\n",
      "\n",
      "sumLogPiRem for doc 2, K[d]=6\n",
      "------\n",
      "2and3 :  [ 0.    0.    0.    0.    0.   -6.95]\n",
      " only2:  [ 0.    0.   -6.95]\n",
      " only3:  [ 0.   -6.95]\n",
      "\n",
      "sumLogPiRem for doc 3, K[d]=9\n",
      "------\n",
      "2and3 :  [ 0.    0.    0.    0.    0.    0.    0.    0.   -4.42]\n",
      " only2:  [ 0.    0.    0.   -4.42]\n",
      " only3:  [ 0.    0.    0.   -4.42]\n"
     ]
    }
   ],
   "source": [
    "uid_propTotalOrder = np.asarray([0, 200, 201, 300, 301, 202, 203, 302, 303])\n",
    "\n",
    "uids_2 = [200,201,202,203]\n",
    "uids_3 = [300,301,302,303]\n",
    "\n",
    "# Loop over all docs\n",
    "for d in range(Data.nDoc):\n",
    "    # Grab the single document\n",
    "    Data_b = Data.select_subset_by_mask([d])\n",
    "    K_d = 1 + K_d_2[d] + K_d_3[d]\n",
    "    \n",
    "    # Do local and summary step for full combination of both splits\n",
    "    propDocTopicCount_d = np.hstack([curDocTopicCount[d, :1], \n",
    "                                    xDocTopicCount_2[d, :K_d_2[d]],\n",
    "                                    xDocTopicCount_3[d, :K_d_3[d]],\n",
    "                                    ])[np.newaxis,:]\n",
    "    propDoc_beta_d = np.hstack([curDoc_beta[d, :1],\n",
    "                               xDoc_beta_2[d, :K_d_2[d]],\n",
    "                               xDoc_beta_3[d, :K_d_3[d]], \n",
    "                               ])[np.newaxis,:]\n",
    "    propLP_2and3_b = dict(\n",
    "        DocTopicCount=propDocTopicCount_d,\n",
    "        theta =propDocTopicCount_d+propDoc_beta_d,\n",
    "        resp=np.random.rand(1, K_d), # doesnt matter\n",
    "        thetaRem=curThetaRem[d],\n",
    "        )\n",
    "    propSS_2and3_b = calcSummaryStats(Data_b, propLP_2and3_b, doPrecompEntropy=1, doTrackTruncationGrowth=1)\n",
    "    propSS_2and3_b.setUIDs([0] + uids_2[:K_d_2[d]] + uids_3[:K_d_3[d]])\n",
    "    propSS_2and3_b.reorderComps(uids=uid_propTotalOrder[:K_d], fieldsToIgnore=['sumLogPiRemVec'])\n",
    "    if d == 0:\n",
    "        propSS_2and3 = propSS_2and3_b.copy()\n",
    "    else:        \n",
    "        Kextra = propSS_2and3_b.K - propSS_2and3.K\n",
    "        if Kextra > 0:\n",
    "            propSS_2and3.insertEmptyComps(Kextra, uid_propTotalOrder[K_d-Kextra:K_d])            \n",
    "        propSS_2and3 += propSS_2and3_b\n",
    "\n",
    "    # Do local and summary step using only expansion terms from state 2\n",
    "    propLP_newonly_2b = dict(\n",
    "        DocTopicCount=xDocTopicCount_2[d,:K_d_2[d]][np.newaxis,:],\n",
    "        theta=xDocTopicCount_2[d, :K_d_2[d]][np.newaxis,:] + alpha * xDoc_beta_2[d,:K_d_2[d]][np.newaxis,:],\n",
    "        resp=np.random.rand(1, K_d_2[d]), # doesnt matter\n",
    "        thetaRem=curThetaRem[d],\n",
    "        )\n",
    "    propLP_newonly_2b['digammaSumTheta'] = np.asarray([curLP['digammaSumTheta'][d]])\n",
    "    propSS_newonly_2b = calcXSummaryStats(Data_b, propLP_newonly_2b, uids=uids_2[:K_d_2[d]],\n",
    "                                          doPrecompEntropy=1, doTrackTruncationGrowth=1)\n",
    "    # Do local and summary step using only expansion terms from state 3\n",
    "    propLP_newonly_3b = dict(\n",
    "        DocTopicCount=xDocTopicCount_3[d,:K_d_3[d]][np.newaxis,:],\n",
    "        theta=xDocTopicCount_3[d,:K_d_3[d]][np.newaxis,:] + alpha * xDoc_beta_3[d,:K_d_3[d]][np.newaxis,:],\n",
    "        resp=np.random.rand(1, K_d_3[d]), # doesnt matter\n",
    "        thetaRem=curThetaRem[d],\n",
    "        )\n",
    "    propLP_newonly_3b['digammaSumTheta'] = np.asarray([curLP['digammaSumTheta'][d]])\n",
    "    propSS_newonly_3b = calcXSummaryStats(Data_b, propLP_newonly_3b,  uids=uids_3[:K_d_3[d]],\n",
    "                                          doPrecompEntropy=1, doTrackTruncationGrowth=1)\n",
    "    if d == 0:\n",
    "        propSS_newonly_2 = propSS_newonly_2b.copy()\n",
    "        propSS_newonly_3 = propSS_newonly_3b.copy()\n",
    "    else:\n",
    "        Kextra = propSS_newonly_2b.K - propSS_newonly_2.K\n",
    "        if Kextra > 0:\n",
    "            propSS_newonly_2.insertEmptyComps(Kextra, newuids=uids_2[K_d_2[d-1]:K_d_2[d]])\n",
    "        propSS_newonly_2 += propSS_newonly_2b\n",
    "        \n",
    "        Kextra = propSS_newonly_3b.K - propSS_newonly_3.K\n",
    "        if Kextra > 0:\n",
    "            propSS_newonly_3.insertEmptyComps(Kextra, newuids=uids_3[K_d_3[d-1]:K_d_3[d]])\n",
    "        propSS_newonly_3 += propSS_newonly_3b\n",
    "        \n",
    "    print ''\n",
    "    print 'sumLogPiRem for doc %d, K[d]=%d' % (d+1, K_d)\n",
    "    print '------'\n",
    "    print '2and3 : ', propSS_2and3_b.sumLogPiRemVec\n",
    "    print ' only2: ', propSS_newonly_2b.sumLogPiRemVec\n",
    "    print ' only3: ', propSS_newonly_3b.sumLogPiRemVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 200 201 300 301 202 203 302 303]\n",
      "[  0 200 201 300 301 202 203 302 303]\n"
     ]
    }
   ],
   "source": [
    "propSS_directFromLP = propSS_2and3\n",
    "print propSS_directFromLP.uids\n",
    "print uid_propTotalOrder\n",
    "assert np.allclose(uid_propTotalOrder, propSS_directFromLP.uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -3.58  -58.05 -108.66  -75.62  -42.33   -3.46   -1.93   -2.26   -2.26]\n",
      "[ 0.    0.    0.    0.   -6.26 -6.95  0.    0.   -4.42]\n",
      "[ 0.   -6.26 -6.95 -4.42]\n"
     ]
    }
   ],
   "source": [
    "print propSS_2and3.sumLogPi\n",
    "print propSS_2and3.sumLogPiRemVec\n",
    "\n",
    "print propSS_newonly_2.sumLogPiRemVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Create sufficient stats for combined proposal by manipulating tracked stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "propSS_fromXSS = curSS.copy()\n",
    "propSS_fromXSS.replaceCompWithExpansion(uid=1, xSS=propSS_newonly_2)\n",
    "propSS_fromXSS.replaceCompWithExpansion(uid=2, xSS=propSS_newonly_3)\n",
    "propSS_fromXSS.reorderComps([0,1,2,5,6,3,4,7,8])\n",
    "assert np.allclose(propSS_fromXSS.uids, uid_propTotalOrder)\n",
    "assert np.allclose(propSS_directFromLP.uids, uid_propTotalOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumLogPi\n",
      "  [  -3.58  -58.05 -108.66  -75.62  -42.33   -3.46   -1.93   -2.26   -2.26]  direct construction proposal\n",
      "  [  -3.58  -58.05 -108.66  -75.62  -42.33   -3.46   -1.93   -2.26   -2.26]  from tracked expansion stats\n",
      "gammalnTheta\n",
      "  [ 162.71   20.19   18.36    7.31   10.97   55.56    1.84    0.72    0.72]  direct construction proposal\n",
      "  [ 162.71   20.19   18.36    7.31   10.97   55.56    1.84    0.72    0.72]  from tracked expansion stats\n",
      "gammalnSumTheta\n",
      "  438.778493399  direct construction proposal\n",
      "  438.778493399  from tracked expansion stats\n",
      "slackTheta\n",
      "  [ 0.64  1.41  1.25  2.69  1.37  0.07  0.08  0.07  0.07]  direct construction proposal\n",
      "  [ 0.64  1.41  1.25  2.69  1.37  0.07  0.08  0.07  0.07]  from tracked expansion stats\n",
      "slackThetaRem\n",
      "  8.37808027748  direct construction proposal\n",
      "  8.37808027748  from tracked expansion stats\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=100, precision=2)\n",
    "for key in ['sumLogPi', 'gammalnTheta', 'gammalnSumTheta', 'slackTheta', 'slackThetaRem']:\n",
    "    print key\n",
    "    if hasattr(propSS_fromXSS, key):\n",
    "        arr_directFromLP = getattr(propSS_directFromLP, key)\n",
    "        arr_fromXSS = getattr(propSS_fromXSS, key)\n",
    "        \n",
    "    elif propSS_fromXSS.hasELBOTerm(key):\n",
    "        arr_directFromLP = propSS_directFromLP.getELBOTerm(key)\n",
    "        arr_fromXSS = propSS_fromXSS.getELBOTerm(key)\n",
    "        \n",
    "    print '  %s  direct construction proposal' % (arr_directFromLP)\n",
    "    print '  %s  from tracked expansion stats' % (arr_fromXSS)\n",
    "    assert np.allclose(arr_fromXSS, arr_directFromLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.  ,  0.  ,  0.  , -6.26, -6.95,  0.  ,  0.  , -4.42])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propSS_directFromLP.sumLogPiRemVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  , -13.21,   0.  ,  -4.42])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propSS_fromXSS.sumLogPiRemVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is a complication in tracking how the truncation varies across batches. It's not possible to obtain the right statistics purely by keeping an incrementally aggregated whole-dataset statistics for each split separately, and then merging.\n",
    "\n",
    "However, summing over all batches would do the right thing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
