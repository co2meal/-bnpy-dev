{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## GOAL\n",
      "\n",
      "Documentation of mathematics and notation for DiagGaussObsModel variational updates.\n",
      "\n",
      "## NOTATION\n",
      "\n",
      "$K$ component mixture model. For each global cluster $k$, we have a mean vector $\\mu_k$ and a precision vector $\\lambda_k$ \n",
      "\n",
      "$$\n",
      "\\mu_k = [ \\mu_{k1} \\ldots \\mu_{kD} ]\n",
      "\\\\\n",
      "\\lambda_k = [\\lambda_{k1} \\ldots \\lambda_{kD}]\n",
      "$$\n",
      "\n",
      "All clusters come from a joint Gauss-Gamma prior distribution.\n",
      "\n",
      "\\begin{align}\n",
      "p(\\mu_k, \\lambda_k) &= \\mbox{GaussGamma}(\\mu_k, \\lambda_k \\mid a_0, b_0, m_0, \\kappa_0)\n",
      "\\\\\n",
      "&= \\mbox{Gamma}(\\lambda_k \\mid a_0, b_0) \\mbox{Normal}(\\mu_k \\mid m_0, (\\kappa_0 \\lambda_k)^{-1})\n",
      "\\end{align}\n",
      "\n",
      "Each cluster has a separate posterior factor $q$.\n",
      "\n",
      "\\begin{align}\n",
      "q(\\mu, \\lambda) &= \\prod_{k=1}^K q(\\mu_k, \\lambda_k)\n",
      "\\\\\n",
      "q(\\mu_k, \\lambda_k) &= \\mbox{GaussGamma}(\\mu_k, \\lambda_k \\mid a_k, b_k, m_k, \\kappa_k)\n",
      "\\\\\n",
      "&= \\mbox{Gamma}(\\lambda_k \\mid a_k, b_k) \\mbox{Normal}(\\mu_k \\mid m_k, (\\kappa_k \\lambda_k)^{-1})\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ELBO Overview\n",
      "\n",
      "The ELBO has three terms related to the cluster parameters $\\{\\mu_k, \\lambda_k\\}$. \n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(q) = \\mbox{E}[ \\log p(x \\mid z, \\mu, \\lambda) + \\log p(\\mu, \\lambda) - \\log q(\\mu, \\lambda) ]\n",
      "\\end{align}\n",
      "\n",
      "These are the data term, prior term, and entropy term. \n",
      "\n",
      "### Data term\n",
      "\n",
      "\\begin{align}\n",
      "\\log p( x \\mid z, \\mu, \\lambda) \n",
      "&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\log p( x_n \\mid \\mu_k, \\lambda_k)\n",
      "\\\\\n",
      "&= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\Big[ - \\frac{D}{2}\\log 2\\pi + \\frac{1}{2} \\sum_{d=1}^D \\log \\lambda_{kd} - \\lambda_{kd} (x_{nd} - \\mu_{kd})^2 \\Big]\n",
      "\\\\\n",
      "&=  - \\frac{ND}{2}\\log 2\\pi + \\frac{1}{2}\\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\Big[ \\sum_{d=1}^D \\log \\lambda_{kd} - \\Big( \\lambda_{kd} x_{nd}^2 - 2\\lambda_{kd} \\mu_{kd} x_{nd} + \\lambda_{kd} \\mu_{kd}^2) \\Big) \\Big]\n",
      "\\\\\n",
      "\\end{align}\n",
      "\n",
      "Now taking expectations and plugging in summaries instead of the sums over data items $n$.\n",
      "\\begin{align}\n",
      "\\mbox{E} \\log p( x \\mid z, \\mu, \\lambda) \n",
      "&=  - \\frac{ND}{2}\\log 2\\pi +\n",
      "\\frac{1}{2} \\sum_{k=1}^K \n",
      "\\sum_{d=1}^D\n",
      " \\mbox{E}[\\log \\lambda_{kd}] N_k - \n",
      " \\Big( \\mbox{E}[\\lambda_{kd}] S_{kd}^2 - 2 \\mbox{E}[\\lambda_{kd} \\mu_{kd}] S_{kd} + \\mbox{E}[\\lambda_{kd} \\mu_{kd}^2] N_k \\Big)\n",
      "\\\\\n",
      "\\end{align}\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}