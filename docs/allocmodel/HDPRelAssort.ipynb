{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\newcommand{\\E}{\\mathbb{E}_q}\n",
      "\\newcommand{\\sume}{\\sum^{E}_{ij}}\n",
      "\\newcommand{\\sumn}{\\sum^{N\\timesN}_{i,j}}\n",
      "\\newcommand{\\sumij}{\\sum^N_{i}\\sum^N_{j\\neq i}}\n",
      "\\newcommand{\\sumi}{\\sum^N_{i=1}}\n",
      "\\newcommand{\\sumj}{\\sum^N_{j=1}}\n",
      "\\newcommand{\\sumk}{\\sum^{K}_{k=1}}\n",
      "\\newcommand{\\sumt}{\\sum^{K^2}_{t=1}}\n",
      "\\newcommand{\\suml}{\\sum^K_{\\ell=1}}\n",
      "\\newcommand{\\sumkl}{\\sum^{K}_{k\\ell}}\n",
      "\\newcommand{\\etail}{\\mathbb{E}_q[\\log (1-v_{ki})]}\n",
      "\\newcommand{\\ehead}{\\mathbb{E}_q[\\log (v_{ki})]}\n",
      "\\newcommand{\\wtail}{\\mathbb{E}_q[\\log (1-w_{k})]}\n",
      "\\newcommand{\\whead}{\\mathbb{E}_q[\\log (w_{k})]}\n",
      "\\newcommand{\\wtaila}{\\mathbb{E}_q[\\log (1-w_{k})]}\n",
      "\\newcommand{\\wheada}{\\mathbb{E}_q[\\log (w_{k})]}\n",
      "\\newcommand{\\wheadae}{\\mathbb{E}_q[\\log (w_{\\epsilon})]}\n",
      "\\newcommand{\\wtailae}{\\mathbb{E}_q[\\log (1-w_{\\epsilon})]}\n",
      "\\newcommand{\\epi}{\\mathbb{E}_q[\\log (\\pi_{ik})]}\n",
      "\\newcommand{\\epij}{\\mathbb{E}_q[\\log (\\pi_{jk})]}\n",
      "\\newcommand{\\epik}{\\mathbb{E}_q[\\log (\\pi_{ik^\\prime})]}\n",
      "\\newcommand{\\epjk}{\\mathbb{E}_q[\\log (\\pi_{jk^{\\prime}})]}\n",
      "\\newcommand{\\epikk}{\\mathbb{E}_q[\\log (\\pi_{ik^{\\prime\\prime}})]}\n",
      "\\newcommand{\\epjkk}{\\mathbb{E}_q[\\log (\\pi_{jk^{\\prime\\prime}})]}\n",
      "\\newcommand{\\epil}{\\mathbb{E}_q[\\log (\\pi_{j\\ell})]}\n",
      "\\newcommand{\\sumknl}{\\sum^{K}_{k=1}\\sum^K_{\\ell\\neq k}}\n",
      "$$\n",
      "\n",
      "## Goal\n",
      "\n",
      "This is documentation on some of the subtle aspects regarding the computation of the ELBO in the assortative HDP Relational Model. The issue here is that due to our efficient structured mean-field inference approach, we need to also calculate our ELBO terms in $O(K)$ rather than $O(K^2)$, which would represent a naive approach.\n",
      "\n",
      "## Notation\n",
      "Let $\\phi_{ij}$ represent our variational posterior for our blocked soft edge community assignments (or local responsibilities) of dimension $E \\times K$, where $E$ is the number of distinct edges that we are training on. Here $\\theta_{ik}$ refers to the mixed-membership community weights for a node $i$ and community $k$. For this model, $\\theta$ is treated as a global variable as well as $[\\lambda_a,\\lambda_b]$ which represents the variational posteriors for our stochastic block matrix.\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  \\whead &= \\psi(\\lambda_{ka}) - \\psi(\\lambda_{k a}+\\lambda_{kb})\\\\\n",
      "  \\wtail &= \\psi(\\lambda_{kb}) - \\psi(\\lambda_{k a}+\\lambda_{kb})\\\\\n",
      "  \\epi &= \\psi(\\theta_{ik}) - \\psi(\\textstyle\\sum_{\\ell=1}^{K+1}\\theta_{i\\ell}) \\\\\n",
      "  \\tilde{\\pi}_{ik} &\\triangleq \\exp \\{ \\epi \\}\\\\\n",
      "  \\tilde{\\pi}_i &\\triangleq \\sum_{k=1}^{K} \\tilde{\\pi}_{ik} \\\\\n",
      "  f(w_k,y_{ij}) &= \\exp\\{ y_{ij}\\whead + (1-y_{ij})\\wtail \\} \\\\\n",
      "  f(\\epsilon,y_{ij}) &= \\exp\\{ y_{ij}\\log(\\epsilon) + (1-y_{ij})\\log(1-\\epsilon) \\}\n",
      "\\end{align}\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Local posterior updates (E-step)\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  \\phi_{ijkk} &\\propto \\tilde{\\pi}_{ik} \\tilde{\\pi}_{jk} f(w_k,y_{ij}) \\\\\n",
      "  \\phi_{ijk\\ell} &\\propto \\tilde{\\pi}_{ik} \\tilde{\\pi}_{j\\ell} f(\\epsilon,y_{ij}), \\quad \\ell \\neq k.\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## M-step updates\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\lambda_{ka} &= \\textstyle \\tau_a + \\sume  \\phi_{ijkk}y_{ij} \\\\\n",
      "\\lambda_{kb} &= \\textstyle \\tau_b + \\sume \\phi_{ijkk}(1-y_{ij}) \\\\\n",
      "\\theta_{ik} &= \\alpha\\beta_k + \\textstyle\\sum_{(i,j)\\in E} \\sum_{\\ell=1}^K \\phi_{ijk\\ell}.\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "Efficient M-Step update that doesn't require non-assortative likelihoods where $Z_{ij}$ represents our normalization constant for edge ij.\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\theta_{ik} &= \\textstyle  \\alpha\\beta_k + \\sum_{(i,j)\\in E} \\phi_{ijkk} + \\frac{1}{Z_{ij}}\n",
      "  \\tilde{\\pi}_{ik} f(\\epsilon,y_{ij})(\\tilde{\\pi}_j - \\tilde{\\pi}_{jk})\\\\\n",
      "   Z_{ij} &= \\tilde{\\pi}_i \\tilde{\\pi}_j f(\\epsilon,y_{ij}) +\n",
      "  \\textstyle  \\sum^K_{k=1} \\tilde{\\pi}_{ik} \\tilde{\\pi}_{jk} \n",
      "  (f(w_k,y_{ij}) - f(\\epsilon,y_{ij}))\n",
      "\\end{align}\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ELBO\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\mathcal{L}(q) \n",
      "&= \\sume \\sum^{K}_{k=1} \\Bigg [\\phi_{ijkk}\\log f(w_k,y_{ij})   \\Bigg ] + \\sume \\Bigg [1-(\\sumk \\phi_{ijkk})\\log f(\\epsilon,y_{ij}) \\Bigg ]  \\\\\n",
      "&+ \\sume \\sum^K_{k=1} \\sum^K_{\\ell=1} \\Bigg [ \\phi_{ijk\\ell} (\\epi + \\epil)  \\Bigg ] \\\\\n",
      "&+ \\sumk (\\gamma - 1)\\log(1-v_k) + \\sum^N_{i=1} \\left [ \\log \\Gamma(\\sumk \\alpha\\beta_k) - \\sumk \\log \\Gamma(\\alpha \\beta_k) + \\sumk (\\alpha\\beta_k -1) \\E[\\log \\pi_{ik}] \\right ]\\\\\n",
      "&+ \\sumk  \\left[ \\log \\left( \\frac{\\Gamma(\\tau_{a}+\\tau_{b})}{\\Gamma(\\tau_a)\\Gamma(\\tau_{b})} \\right )+(\\tau_{a}-1)\\wheada + (\\tau_{b}-1)\\wtaila\\right ]\\\\\n",
      "&- \\sume\\sum^{K}_{k=1} \\sum^K_{\\ell=1}   \\phi_{ijk\\ell} \\log(\\phi_{ijk\\ell})\\\\\n",
      "&- \\sumi \\left [ \\log \\Gamma(\\sumk \\theta_{ik}) - \\sumk \\log \\Gamma(\\theta_{ik}) + \\sumk(\\theta_{ik}-1)\\E[\\log \\pi_{ik}] \\right ]\\\\\n",
      "&- \\sumk \\left [ \\log \\left( \\frac{\\Gamma(\\lambda_{ka}+\\lambda_{kb})}{\\Gamma(\\lambda_{ka})\\Gamma(\\lambda_{kb})} \\right) + (\\lambda_{ka}-1)\\wheada + (\\lambda_{kb}-1)\\wtaila \\right ]\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "Note in particular the terms $\\E[\\log p(e_{ij}| \\pi_i,\\pi_j)] - \\E[\\log q(e_{ij}| \\phi_{ij})]$ within our elbo which refer to our local assignment variables. We will show below how several of these terms cancel out with one another to make for a highly efficient ELBO computation.\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\E[\\log q(e_{ij}| \\phi_{ij})] \n",
      "&= \\sum^{K}_{k=1} \\sum^K_{\\ell=1}   \\phi_{ijk\\ell} \\log(\\phi_{ijk\\ell})  \\\\\n",
      "&= \\sum^{K}_{k=1} \\phi_{ijkk} \\log(\\phi_{ijkk}) + \\sum^{K}_{k=1}\\sum^K_{\\ell\\neq k}  \\phi_{ijk\\ell} \\log(\\phi_{ijk\\ell})\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "We now consider the portion of the entropy associated with our epsilon likelihoods (i.e $\\sum^{K}_{k=1}\\sum^K_{\\ell\\neq k}\\phi_{ijk\\ell} \\log(\\phi_{ijk\\ell})$) and show how that will cancel with similar terms in $\\E[\\log p(e_{ij}| \\pi_i,\\pi_j)]$. Focusing on the entropy for a single edge we have:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\sumknl \\phi_{ijk\\ell} \\log(\\phi_{ijk\\ell}) \n",
      "&= \\sumknl \\phi_{ijk\\ell} \\log \\big ( \\frac{\\tilde{\\pi}_{ik} \\tilde{\\pi}_{j\\ell} f(\\epsilon,y_{ij})}{Z_{ij}} \\big ) \\\\\n",
      "&= \\sumknl \\phi_{ijk\\ell} \\times \\big( \\epi + \\epil + \\log(f(\\epsilon,y_{ij})) - \\log(Z_{ij}) \\big) \\\\\n",
      "&= \\sumknl \\phi_{ijk\\ell} \\times \\big( \\epi + \\epil \\big ) + \\sumknl \\phi_{ijk\\ell} \\times \\big ( \\log(f(\\epsilon,y_{ij})) - \\log(Z_{ij}) \\big)\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "Now note that the first term of this entropy can be subtracted from all $\\phi_{ijk\\ell}$ where $k \\neq \\ell$ within $\\E[\\log p(e_{ij}| \\pi_i,\\pi_j)]$. This results in a significantly more efficient computation of our ELBO:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\E[\\log p(e_{ij}| \\pi_i,\\pi_j)] - \\E[\\log q(e_{ij}| \\phi_{ij})] \n",
      "&=  \\sumk \\phi_{ijkk} (\\epi + \\E[\\log \\pi_{jk}]) - \\sumk \\phi_{ijkk} \\log(\\phi_{ijkk}) \n",
      "- \\sumknl \\phi_{ijk\\ell} \\times \\big ( \\log(f(\\epsilon,y_{ij})) - \\log(Z_{ij}) \\big)\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Experiment for Debugging - Sanity Check"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Command to execute code on toy graph with 75 nodes and 5 true communities:\n",
      "\n",
      "\"BinaryGraphK5 HDPRelAssortModel BernRel VB --K=5 --nLap=100\"\n",
      "\n",
      "Output should look something like this:\n",
      "\n",
      "<img src=\"files/files/results_toy.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}