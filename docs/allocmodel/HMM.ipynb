{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Goal\n",
      "\n",
      "This is documentation for **HMM**, a finite, parametric hidden Markov model with $K$ components/states.\n",
      "\n",
      "This document only addresses *allocation*, which means we will only describe notations and update equations that pertain to assigning data items to clusters/components.  Anything to do with the likelihood funciton is covered in the appropriate **obsmodel** documentation.\n",
      "\n",
      "## Notation\n",
      "\n",
      "### Parameters\n",
      "\n",
      "An HMM has two key allocation parameters: vector $\\pi_0$ and square matrix $\\pi$.\n",
      "\n",
      "Let vector $\\pi_0$ denote the probability of starting the sequence at each of $K$ states\n",
      "$$\n",
      "\\pi_0 = [\\pi_{01} ~ \\pi_{02} ~ \\ldots \\pi_{0K}]\n",
      "$$\n",
      "\n",
      "Let matrix $\\pi$ have size $K\\times K$. Each row $\\pi_j$ is the probability of transitioning from state $j$ to each of the $K$ states.\n",
      "$$\n",
      "\\pi_j = [\\pi_{j1} ~ \\pi_{j2} ~ \\ldots \\pi_{jK}]\n",
      "$$\n",
      "\n",
      "Each vector $\\pi_{0}, \\pi_{1}, \\ldots \\pi_{K}$ satisfies two constraints\n",
      "* each entry is positive:  $\\pi_{jk} > 0$\n",
      "* the vector sums to one: $\\sum_{k=1}^K \\pi_{jk} = 1$.\n",
      "\n",
      "### Prior parameters\n",
      "\n",
      "When desired, we place a symmetric Dirichlet prior on each row of $\\pi$, with parameter $\\alpha_0$.\n",
      "\n",
      "$$\n",
      "p( \\pi_j \\mid \\alpha_0) = \\mbox{Dirichlet}( \\alpha_0, \\alpha_0, \\ldots \\alpha_0)\n",
      "$$\n",
      "\n",
      "Here, $\\alpha_0$ is a positive scalar: $\\alpha_0 > 0$.\n",
      "\n",
      "TODO: optional sticky parameter?\n",
      "\n",
      "## Local parameters: discrete assignments\n",
      "\n",
      "We let $z_{n}$ denote the state sequence of the $n$-th observed sequence. This sequence has length $T_n$.\n",
      "\n",
      "$$\n",
      "z_{n} = [ z_{n1} z_{n2} \\ldots z_{n T_n-1} ~ z_{nT_n} ]\n",
      "$$\n",
      "\n",
      "Here, we'll use *one-hot* notation, in which $z_{nt}$ is a binary vector of length $K$.\n",
      "\n",
      "$$\n",
      "z_{nt} = [ z_{nt1} z_{nt2} \\ldots z_{ntK} ]\n",
      "\\\\\n",
      "z_{ntk} \\in \\{0, 1\\}\n",
      "$$\n",
      "\n",
      "This vector is constrained so only *one* entry of $z_n$ is one, and the rest are zeros.\n",
      "\n",
      "The event $z_{ntk} = 1$ can be interpreted as \"timestep $t$ in sequence $n$ is assigned to cluster $k$.\"\n",
      "\n",
      "## Local Posterior\n",
      "\n",
      "We would like to be able to estimate $p(z_n | X, \\pi, \\phi)$, where $X$ is the full observed data set, and $\\phi$ is the set of parameters for the observation model.  This will be a categorical distribution over the $K$ possible values of $z_n$:\n",
      "\n",
      "$$ p(z_n | X, \\pi, \\phi) = \\mathrm{Cat}(z_n | r_{n1}, r_{n2}, \\dots , r_{nK}) $$\n",
      "\n",
      "## Maximum Likelihood\n",
      "\n",
      "The problem of finding a $\\pi$ such that $p(X | \\pi)$ does not have a closed form solution.  This problem, however, can be solved iteratively by the EM algorithm.  In the E step, the parameter $\\pi$ is held fixed and the local posterior distrubitions are calculated for each $z_n$.  In the M step, the global parameters $\\pi$ are maximized using the posteriors calculated in the E step.\n",
      "\n",
      "\n",
      "## Local Posterior Updates (E-Step)\n",
      "\n",
      "For reasons that will become clear when looking at the M step, define $\\gamma(z_{nk}) := p(z_{nk} | X, \\pi)$ and $\\psi(z_{nk}, z_{n-1,j}) := p(z_{nk}, z_{n-1, j} | X, \\pi)$.  Since everything is conditioned on the parameters $\\pi$, we'll just drop it to save on clutter.  Now, defining $\\alpha(z_n) := p(x_1, \\dots, x_n, z_n)$ and $\\beta(z_n) := p(x_{n+1}, \\dots, x_N | z_n)$, $\\gamma(z_n) can be written:\n",
      "\n",
      "$$ \\gamma(z_n) = \\frac{\\alpha(z_n) \\beta(z_n)}{p(X)} $$\n",
      "\n",
      "Now we just need a way to compute $\\alpha$ and $\\beta$.  \n",
      "\n",
      "### Calculation of $\\alpha$ and $\\beta$\n",
      "\n",
      "By doing some manipulation, a recursive definition of $\\alpha$ can be found:\n",
      "\n",
      "$$ \\alpha(z_n) = p(x_n | z_n) \\sum_{z_{n-1}} \\alpha(z_{n-1}) p(z_n | z_{n-1})$$\n",
      "\n",
      "Where the final equality holds simply by definition of $\\alpha$.  Note that the terms $p(x_n | z_n)$ and $p(z_n | z_{n-1})$ are immediately computable from the observation model and transition matrix, so that each step of this recursion is easily computable.  The base case is given, by definition, as $\\alpha(z_1) = p(z_1)p(x_1 | z_1) = \\pi_{0k}p(x_1 | z_1)$.\n",
      "\n",
      "A recursive formula for $\\beta$ can also be found:\n",
      "\n",
      "$$ \\beta(z_n) = \\sum_{z_{n+1}} \\beta(z_{n+2}) p(x_{n+1} | z_{n+1}) p(z_{n+1} | z_n) $$\n",
      "\n",
      "For the base case of this recursion, note that:\n",
      "\n",
      "$$ p(z_N | X) = \\frac{\\alpha(z_N)\\beta(z_N)}{p(X)} = \\frac{p(X, z_N)\\beta(z_N)}{p(X)} $$\n",
      "\n",
      "By Bayes' law, it must be that $\\beta(z_N) = 1$, which gives the base case for the recursion.\n",
      "\n",
      "As the recursion of $\\alpha$ runs \"forward\" through the data, and the recursion of $\\beta$ runs \"backwards,\" this process is called the forward-backward algorithm.\n",
      "\n",
      "## Global Parameter Updates (M-Step)\n",
      "\n",
      "In the M-step, we find parameter values $\\pi$ that maximize the expected complete data log-likelihood (i.e. $E(\\log p(X, Z | \\pi))$) using the posterior distributions that we calculated in the E-step.  That is, we want to maximize:\n",
      "\n",
      "$$ L(\\pi) = \\sum_{Z} p(Z, X |  \\pi^{\\text{old}}) \\log p(X, Z | \\pi) \\propto \\sum_{Z} p(Z | X, \\pi^{\\text{old}}) \\log p(X, Z | \\pi)$$\n",
      "\n",
      "where $\\pi^{\\text{old}}$ are the parameters that were calculated in the previous M-step.  Plugging in the specifics of HMM's and $\\gamma$ and $\\psi$ with the constraints $\\sum_k=1^K \\pi_{0k} = 1$ gives, for each $k$:\n",
      "\n",
      "$$ \\pi_{0k} = \\frac{\\gamma(z_{1k})}{\\sum_k \\gamma(z_{1k})} $$\n",
      "\n",
      "$$ \\pi_{jk} = \\frac{\\sum_{n=2}^N \\psi(z_{n-1,j}, z_{nk})}{\\sum_{l=1}^K \\sum_{n=2}^N \\psi(z_{n-1,j}, z_{nl})} $$\n",
      "\n",
      "where the second equation holds for $1 \\leq j \\leq K$.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Local posterior\n",
      "\n",
      "TODO"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}