{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Goal\n",
      "\n",
      "This is documentation for **HMM**, a finite, parametric hidden Markov model with $K$ components/states.\n",
      "\n",
      "This document only addresses *allocation*, which means we will only describe notations and update equations that pertain to assigning data items to clusters/components.  Anything to do with the likelihood funciton is covered in the appropriate **obsmodel** documentation.\n",
      "\n",
      "## Notation\n",
      "\n",
      "### Parameters\n",
      "\n",
      "An HMM has two key allocation parameters: vector $\\pi_0$ and square matrix $\\pi$.\n",
      "\n",
      "Let vector $\\pi_0$ denote the probability of starting the sequence at each of $K$ states\n",
      "$$\n",
      "\\pi_0 = [\\pi_{01} ~ \\pi_{02} ~ \\ldots \\pi_{0K}]\n",
      "$$\n",
      "\n",
      "Let matrix $\\pi$ have size $K\\times K$. Each row $\\pi_j$ is the probability of transitioning from state $j$ to each of the $K$ states.\n",
      "$$\n",
      "\\pi_j = [\\pi_{j1} ~ \\pi_{j2} ~ \\ldots \\pi_{jK}]\n",
      "$$\n",
      "\n",
      "Each vector $\\pi_{0}, \\pi_{1}, \\ldots \\pi_{K}$ satisfies two constraints\n",
      "* each entry is positive:  $\\pi_{jk} > 0$\n",
      "* the vector sums to one: $\\sum_{k=1}^K \\pi_{jk} = 1$.\n",
      "\n",
      "### Prior parameters\n",
      "\n",
      "When desired, we place a symmetric Dirichlet prior on each row of $\\pi$, with parameter $\\alpha_0$.\n",
      "\n",
      "$$\n",
      "p( \\pi_j \\mid \\alpha_0) = \\mbox{Dirichlet}( \\alpha_0, \\alpha_0, \\ldots \\alpha_0)\n",
      "$$\n",
      "\n",
      "Here, $\\alpha_0$ is a positive scalar: $\\alpha_0 > 0$.\n",
      "\n",
      "TODO: optional sticky parameter?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Local parameters: discrete assignments\n",
      "\n",
      "We let $z_{n}$ denote the state sequence of the $n$-th observed sequence. This sequence has length $T_n$.\n",
      "\n",
      "$$\n",
      "z_{n} = [ z_{n1} z_{n2} \\ldots z_{n T_n-1} ~ z_{nT_n} ]\n",
      "$$\n",
      "\n",
      "Here, we'll use *one-hot* notation, in which $z_{nt}$ is a binary vector of length $K$.\n",
      "\n",
      "$$\n",
      "z_{nt} = [ z_{nt1} z_{nt2} \\ldots z_{ntK} ]\n",
      "\\\\\n",
      "z_{ntk} \\in \\{0, 1\\}\n",
      "$$\n",
      "\n",
      "This vector is constrained so only *one* entry of $z_n$ is one, and the rest are zeros.\n",
      "\n",
      "The event $z_{ntk} = 1$ can be interpreted as \"timestep $t$ in sequence $n$ is assigned to cluster $k$.\"\n",
      "\n",
      "## Local Posterior\n",
      "\n",
      "For Bayesian analysis, we care about two particular marginal posteriors over local assignments of timesteps to clusters.\n",
      "\n",
      "#### Single-timestep posterior: $p( z_{ntk} = 1 | x_n, \\pi, \\phi)$\n",
      "\n",
      "This is the probability that timestep $t$ is assigned to state $k$, given the entire observed sequence $x_n$. \n",
      "\n",
      "$$ \n",
      "p(z_{nt} | x_n, \\pi, \\phi) = \\mathrm{Discrete}( z_{nt} | r_{nt1}, r_{nt2}, \\dots , r_{ntK}) \n",
      "$$\n",
      "\n",
      "Concretely, this is a discrete distribution over $K$ possible states. We must learn parameter vector $r_{nt}$, which obeys two constraints\n",
      "\n",
      "* non-negative entries: $r_{ntk} \\ge 0$\n",
      "* vector sums to one: $\\sum_{k=1}^K r_{ntk} = 1$\n",
      "\n",
      "#### Adjacent-pair posterior:  $p( z_{n,t-1,j} = 1, z_{ntk} = 1 | x_n, \\pi, \\phi)$\n",
      "\n",
      "In plain English, this is the probability of assigning state $j$ followed by state $k$ at timesteps $t-1, t$, given the entire observed sequence $x_n$.\n",
      "\n",
      "$$\n",
      "p(z_{nt-1}, z_{nt} | x_n, \\pi, \\phi) = \\mathrm{Discrete}( z_{n,t-1}, z_{nt} | s_{nt11}, s_{nt12} \\dots, s_{nt1K}, \\ldots s_{ntK1}, \\ldots s_{ntKK}) \n",
      "$$\n",
      "\n",
      "Concretely, this is a discrete distribution over $K^2$ possible states. We must learn parameter vector $s_{nt}$, which obeys two constraints\n",
      "\n",
      "* non-negative entries: $s_{ntjk} \\ge 0$\n",
      "* vector sums to one: $\\sum_{j=1}^K \\sum_{k=1}^K s_{ntjk} = 1$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Maximum Likelihood\n",
      "\n",
      "The problem of finding a $\\pi$ such that $p(X | \\pi)$ does not have a closed form solution.  This problem, however, can be solved iteratively by the EM algorithm.  In the E step, the parameter $\\pi$ is held fixed and the local posterior distrubitions are calculated for each $z_n$.  In the M step, the global parameters $\\pi$ are maximized using the posteriors calculated in the E step.\n",
      "\n",
      "\n",
      "## Local Posterior Updates (E-Step)\n",
      "\n",
      "For reasons that will become clear when looking at the M step, define $\\gamma(z_{nk}) := p(z_{nk} | X, \\pi)$ and $\\psi(z_{nk}, z_{n-1,j}) := p(z_{nk}, z_{n-1, j} | X, \\pi)$.  We have:\n",
      "\n",
      "$$ \\gamma (z_n) = \\frac{p(X | z_n, \\pi)p(z_n | \\pi)}{p(X | \\pi)} = \\frac{p(x_1, \\dots x_n | z_n, \\pi)p(x_{n+1}, \\dots x_N | z_n, \\pi)p(z_n | \\pi)}{p(X | \\pi)} = \\frac{p(x_1, \\dots, x_n, z_n | \\pi)p(x_{n+1}, \\dots x_N | \\pi)}{p(X | \\pi)}$$\n",
      "\n",
      "Since everything is conditioned on the parameters $\\pi$, we'll just drop it to save on clutter.  Now, defining $\\alpha(z_n) := p(x_1, \\dots, x_n, z_n)$ and $\\beta(z_n) := p(x_{n+1}, \\dots, x_N | z_n)$, $\\gamma(z_n) can be written:\n",
      "\n",
      "$$ \\gamma(z_n) = \\frac{\\alpha(z_n) \\beta(z_n)}{p(X)} $$\n",
      "\n",
      "Now we just need a way to compute $\\alpha$ and $\\beta$.  \n",
      "\n",
      "### Calculation of $\\alpha$\n",
      "\n",
      "A recursive definition for $\\alpha$ can be found as follows:\n",
      "\n",
      "$$ \\alpha(z_n) = p(x_1, \\dots, x_n | z_n)p(z_n) = p(x_n | z_n) p(x_1, \\dots x_{n-1} | z_n) p(z_n) = p(x_1, \\dots x_{n-1} | z_n) p(x_n | z_n) p(z_n) $$\n",
      "\n",
      "Note that the second equality holds by the independence of $x_n$ from all the other data points when conditioned on $z_n$.  This can be further manipulated:\n",
      "\n",
      "$$ = p(x_n | z_n)p(x_1, \\dots x_{n-1}, z_n) = p(x_n | z_n) \\sum_{z_{n-1}} p(x_1, \\dots, x_{n-1}, z_{n-1}, z_n) $$\n",
      "$$ = p(x_n | z_n) \\sum_{z_{n-1}} p(x_1, \\dots x_{n-1} | z_{n-1}, z_n)p(z_{n-1}, z_n)  = p(x_n | z_n) \\sum_{z_{n-1}} p(x_1, \\dots, x_{n-1} | z_{n-1}) p(z_n | z_{n-1}) p(z_{n-1}) $$\n",
      "$$= p(x_n | z_n) \\sum_{z_{n-1}} p(x_1, \\dots, x_{n-1}, z_{n-1}) p(z_n | z_{n-1}) = p(x_n | z_n) \\sum_{z_{n-1}} \\alpha(z_{n-1}) p(z_n | z_{n-1})$$\n",
      "\n",
      "Where the final equality holds simply by definition of $\\alpha$.  Note that the terms $p(x_n | z_n)$ and $p(z_n | z_{n-1})$ are immediately computable from the observation model and transition matrix, so that each step of this recursion is easily computable.  The base case is given, by definition, as $\\alpha(z_1) = p(z_1)p(x_1 | z_1) = \\pi_{0k}p(x_1 | z_1)$.\n",
      "\n",
      "\n",
      "### Calculation of $\\beta$\n",
      "\n",
      "A recursive formula for $\\beta$ can also be found:\n",
      "\n",
      "$$ \\beta(z_n) = p(x_{n+1}, \\dots x_N | z_n) = \\sum_{z_{n+1}} p(x_{n+1}, \\dots x_N, z_{n+1} | z_n) = \\sum_{z_{n+1}} p(x_{n+1}, \\dots x_N | z_n, z_{n+1})p(z_{n+1} | z_n) $$\n",
      "\n",
      "By the structure of a HMM, in the term $p(x_{n+1}, \\dots, x_N | z_n, z_{n+1})$, we can ignore the conditioning on $z_n$.  Further, when conditioned on $z_{n+1}$, $x_{n+1}$ is independent of all the other observations, so:\n",
      "\n",
      "$$ = \\sum_{z_{n+1}} p(x_{n+2}, \\dots , x_N | z_{n+1}) p(x_{n+1} | z_{n+1}) p(z_{n+1} | z_n) $$\n",
      "$$ = \\sum_{z_{n+1}} \\beta(z_{n+2}) p(x_{n+1} | z_{n+1}) p(z_{n+1} | z_n) $$\n",
      "\n",
      "For the base case of this recursion, note that:\n",
      "\n",
      "$$ p(z_N | X) = \\frac{\\alpha(z_N)\\beta(z_N)}{p(X)} = \\frac{p(X, z_N)\\beta(z_N)}{p(X)} $$\n",
      "\n",
      "By Bayes' law, it must be that $\\beta(z_N) = 1$, which gives the base case for the recursion.\n",
      "\n",
      "As the recursion of $\\alpha$ runs \"forward\" through the data, and the recursion of $\\beta$ runs \"backwards,\" this process is called the forward-backward algorithm.\n",
      "\n",
      "## Global Parameter Updates (M-Step)\n",
      "\n",
      "In the M-step, we find parameter values $\\pi$ that maximize the expected complete data log-likelihood (i.e. $E(\\log p(X, Z | \\pi))$) using the posterior distributions that we calculated in the E-step.  That is, we want to maximize:\n",
      "\n",
      "$$ L(\\pi) = \\sum_{Z} p(Z, X |  \\pi^{\\text{old}}) \\log p(X, Z | \\pi) \\propto \\sum_{Z} p(Z | X, \\pi^{\\text{old}}) \\log p(X, Z | \\pi)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Local posterior\n",
      "\n",
      "TODO"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}